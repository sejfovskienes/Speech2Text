{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabefef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import torch \n",
    "import onnxscript\n",
    "from transformers import Speech2TextForConditionalGeneration, Speech2TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ec4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"facebook/s2t-small-librispeech-asr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "221b59d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Speech2TextForConditionalGeneration(\n",
       "  (model): Speech2TextModel(\n",
       "    (encoder): Speech2TextEncoder(\n",
       "      (conv): Conv1dSubsampler(\n",
       "        (conv_layers): ModuleList(\n",
       "          (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "          (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
       "        )\n",
       "      )\n",
       "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Speech2TextEncoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Speech2TextDecoder(\n",
       "      (embed_tokens): Embedding(10000, 256, padding_idx=1)\n",
       "      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x Speech2TextDecoderLayer(\n",
       "          (self_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Speech2TextAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=10000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--- Converting the encoder:\n",
    "processor = Speech2TextProcessor.from_pretrained(MODEL_NAME)\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50746054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_15072\\76168658.py:4: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1122 12:28:29.245000 15072 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
      "W1122 12:28:29.780000 15072 Lib\\site-packages\\torch\\onnx\\_internal\\exporter\\_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Speech2TextEncoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Speech2TextEncoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n",
      "Failed to convert the model to the target version 17 using the ONNX C API. The model was not modified\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 127, in call\n",
      "    converted_proto = _c_api_utils.call_onnx_api(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\onnxscript\\version_converter\\_c_api_utils.py\", line 65, in call_onnx_api\n",
      "    result = func(proto)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\onnxscript\\version_converter\\__init__.py\", line 122, in _partial_convert_version\n",
      "    return onnx.version_converter.convert_version(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\onnx\\version_converter.py\", line 39, in convert_version\n",
      "    converted_model_str = C.convert_version(model_str, target_version)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: D:\\a\\onnx\\onnx\\onnx/version_converter/BaseConverter.h:68: adapter_lookup: Assertion `false` failed: No Adapter From Version $18 for Split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 64 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 18},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.1+cpu',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"input_features\"<FLOAT,[s6,s37,80]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"encoder_outputs\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"conv.conv_layers.1.bias\"<FLOAT,[512]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.0.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.1.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.2.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.3.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.4.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.5.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.6.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.7.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.8.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.9.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.10.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn.k_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn.v_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn.q_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn.out_proj.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.self_attn_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.fc2.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.final_layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layers.11.final_layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layer_norm.weight\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"layer_norm.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"conv.conv_layers.0.weight\"<FLOAT,[1024,80,5]>{TorchTensor(...)},\n",
       "                %\"conv.conv_layers.0.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"conv.conv_layers.1.weight\"<FLOAT,[512,512,5]>{TorchTensor(...)},\n",
       "                %\"embed_positions.weights\"<FLOAT,[6002,256]>{TorchTensor(...)},\n",
       "                %\"layers.0.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.1.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.2.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.3.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.4.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.5.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.6.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.7.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.8.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.9.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.10.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"layers.11.fc1.bias\"<FLOAT,[2048]>{TorchTensor(...)},\n",
       "                %\"val_26\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_29\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_37\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_77\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_81\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_83\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_87\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_89\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_97\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_137\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_141\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_143\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_147\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_149\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_157\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_197\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_201\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_203\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_207\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_209\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_217\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_257\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_261\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_263\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_267\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_269\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_277\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_317\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_321\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_323\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_327\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_329\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_337\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_377\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_381\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_383\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_387\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_389\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_397\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_437\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_441\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_443\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_447\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_449\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_457\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_497\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_501\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_503\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_507\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_509\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_517\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_557\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_561\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_563\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_567\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_569\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_577\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_617\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_621\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_623\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_627\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_629\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_637\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_677\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_681\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_683\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_687\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_689\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_697\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_737\"<FLOAT,[256,256]>{Tensor(...)},\n",
       "                %\"val_741\"<FLOAT,[256,2048]>{Tensor(...)},\n",
       "                %\"val_743\"<FLOAT,[2048,256]>{Tensor(...)},\n",
       "                %\"val_16\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_16')},\n",
       "                %\"val_2\"<INT64,[]>{Tensor<INT64,[]>(array(-1), name='val_2')},\n",
       "                %\"val_3\"<INT64,[]>{Tensor<INT64,[]>(array(4), name='val_3')},\n",
       "                %\"val_4\"<INT64,[]>{Tensor<INT64,[]>(array(1), name='val_4')},\n",
       "                %\"val_5\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(16., dtype=float32), name='val_5')},\n",
       "                %\"val_6\"<INT64,[]>{Tensor<INT64,[]>(array(0), name='val_6')},\n",
       "                %\"val_28\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(0.125, dtype=float32), name='val_28')},\n",
       "                %\"val_34\"<INT64,[1]>{Tensor<INT64,[1]>(array([4]), name='val_34')},\n",
       "                %\"val_35\"<INT64,[1]>{Tensor<INT64,[1]>(array([64]), name='val_35')},\n",
       "                %\"val_75\"<INT64,[1]>{Tensor<INT64,[1]>(array([256]), name='val_75')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"input_features\") {start=0, end=1}\n",
       "              1 |  # node_sym_size_int_103\n",
       "                   %\"sym_size_int_103\"<INT64,[]> ⬅️ ::Squeeze(%\"val_0\")\n",
       "              2 |  # node_Shape_1\n",
       "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"input_features\") {start=1, end=2}\n",
       "              3 |  # node_sym_size_int_104\n",
       "                   %\"sym_size_int_104\"<INT64,[]> ⬅️ ::Squeeze(%\"val_1\")\n",
       "              4 |  # node_transpose\n",
       "                   %\"transpose\"<FLOAT,[s6,80,s37]> ⬅️ ::Transpose(%\"input_features\") {perm=(0, 2, 1)}\n",
       "              5 |  # node_conv1d\n",
       "                   %\"conv1d\"<FLOAT,[s6,1024,(((s37 - 1)//2)) + 1]> ⬅️ ::Conv(%\"transpose\", %\"conv.conv_layers.0.weight\"{...}, %\"conv.conv_layers.0.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1,), strides=(2,), pads=(2, 2)}\n",
       "              6 |  # n0\n",
       "                   %\"first\"<FLOAT,[s6,512,(((s37 - 1)//2)) + 1]>, %\"second\"<FLOAT,[s6,512,(((s37 - 1)//2)) + 1]> ⬅️ ::Split(%\"conv1d\") {axis=1, num_outputs=2}\n",
       "              7 |  # n1\n",
       "                   %\"tmp\"<FLOAT,[s6,512,(((s37 - 1)//2)) + 1]> ⬅️ ::Sigmoid(%\"second\")\n",
       "              8 |  # n2\n",
       "                   %\"glu\"<FLOAT,[s6,512,(((s37 - 1)//2)) + 1]> ⬅️ ::Mul(%\"first\", %\"tmp\")\n",
       "              9 |  # node_conv1d_1\n",
       "                   %\"conv1d_1\"<FLOAT,[s6,512,(((s37 - 1)//4)) + 1]> ⬅️ ::Conv(%\"glu\", %\"conv.conv_layers.1.weight\"{...}, %\"conv.conv_layers.1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1,), strides=(2,), pads=(2, 2)}\n",
       "             10 |  # n0_2\n",
       "                   %\"first_2\"<FLOAT,[s6,256,(((s37 - 1)//4)) + 1]>, %\"second_2\"<FLOAT,[s6,256,(((s37 - 1)//4)) + 1]> ⬅️ ::Split(%\"conv1d_1\") {axis=1, num_outputs=2}\n",
       "             11 |  # n1_2\n",
       "                   %\"tmp_2\"<FLOAT,[s6,256,(((s37 - 1)//4)) + 1]> ⬅️ ::Sigmoid(%\"second_2\")\n",
       "             12 |  # n2_2\n",
       "                   %\"glu_1\"<FLOAT,[s6,256,(((s37 - 1)//4)) + 1]> ⬅️ ::Mul(%\"first_2\", %\"tmp_2\")\n",
       "             13 |  # node_transpose_1\n",
       "                   %\"transpose_1\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Transpose(%\"glu_1\") {perm=(0, 2, 1)}\n",
       "             14 |  # node_add_42\n",
       "                   %\"add_42\"<INT64,[]> ⬅️ ::Add(%\"val_2\"{-1}, %\"sym_size_int_104\")\n",
       "             15 |  # node_floordiv_7\n",
       "                   %\"floordiv_7\"<INT64,[]> ⬅️ ::Div(%\"add_42\", %\"val_3\"{4})\n",
       "             16 |  # node_add_43\n",
       "                   %\"add_43\"<INT64,[]> ⬅️ ::Add(%\"val_4\"{1}, %\"floordiv_7\")\n",
       "             17 |  # node_mul_35\n",
       "                   %\"mul_35\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"transpose_1\", %\"val_5\"{16.0})\n",
       "             18 |  # node_Reshape_9\n",
       "                   %\"val_9\"<INT64,[1]> ⬅️ ::Reshape(%\"add_43\", %\"val_16\"{[-1]}) {allowzero=0}\n",
       "             19 |  # node_Concat_10\n",
       "                   %\"val_10\"<INT64,[2]> ⬅️ ::Concat(%\"val_0\", %\"val_9\") {axis=0}\n",
       "             20 |  # node_zeros\n",
       "                   %\"zeros\"<INT64,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Expand(%\"val_6\"{0}, %\"val_10\")\n",
       "             21 |  # node_Equal_11\n",
       "                   %\"val_11\"<BOOL,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Equal(%\"zeros\", %\"val_4\"{1})\n",
       "             22 |  # node_ne_7\n",
       "                   %\"ne_7\"<BOOL,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Not(%\"val_11\")\n",
       "             23 |  # node__to_copy\n",
       "                   %\"_to_copy\"<INT32,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Cast(%\"ne_7\") {to=6}\n",
       "             24 |  # node_convert_element_type_default\n",
       "                   %\"convert_element_type_default\"<INT64,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Cast(%\"_to_copy\") {to=7}\n",
       "             25 |  # node_cumsum\n",
       "                   %\"cumsum\"<INT64,[1,(((s37 - 1)//4)) + 1]> ⬅️ ::CumSum(%\"convert_element_type_default\", %\"val_4\"{1}) {exclusive=0, reverse=0}\n",
       "             26 |  # node_type_as\n",
       "                   %\"type_as\"<INT32,[1,(((s37 - 1)//4)) + 1]> ⬅️ ::Cast(%\"cumsum\") {to=6}\n",
       "             27 |  # node_mul_50\n",
       "                   %\"mul_50\"<INT32,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Mul(%\"type_as\", %\"_to_copy\")\n",
       "             28 |  # node__to_copy_1\n",
       "                   %\"_to_copy_1\"<INT64,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Cast(%\"mul_50\") {to=7}\n",
       "             29 |  # node_add_76\n",
       "                   %\"add_76\"<INT64,[s6,(((s37 - 1)//4)) + 1]> ⬅️ ::Add(%\"_to_copy_1\", %\"val_4\"{1})\n",
       "             30 |  # node_Reshape_701\n",
       "                   %\"val_17\"<INT64,[None]> ⬅️ ::Reshape(%\"add_76\", %\"val_16\"{[-1]})\n",
       "             31 |  # node_index_select\n",
       "                   %\"index_select\"<FLOAT,[s6*((((s37 - 1)//4)) + 1),256]> ⬅️ ::Gather(%\"embed_positions.weights\"{...}, %\"val_17\") {axis=0}\n",
       "             32 |  # node_Concat_23\n",
       "                   %\"val_23\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_9\", %\"val_16\"{[-1]}) {axis=0}\n",
       "             33 |  # node_view_1\n",
       "                   %\"view_1\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Reshape(%\"index_select\", %\"val_23\") {allowzero=1}\n",
       "             34 |  # node_add_97\n",
       "                   %\"add_97\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"mul_35\", %\"view_1\")\n",
       "             35 |  # node_layer_norm\n",
       "                   %\"layer_norm\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_97\", %\"layers.0.self_attn_layer_norm.weight\"{...}, %\"layers.0.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "             36 |  # node_MatMul_25\n",
       "                   %\"val_27\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_26\"{...})\n",
       "             37 |  # node_linear\n",
       "                   %\"linear\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_27\", %\"layers.0.self_attn.q_proj.bias\"{...})\n",
       "             38 |  # node_mul_81\n",
       "                   %\"mul_81\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear\", %\"val_28\"{0.125})\n",
       "             39 |  # node_MatMul_28\n",
       "                   %\"val_30\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_29\"{...})\n",
       "             40 |  # node_linear_1\n",
       "                   %\"linear_1\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_30\", %\"layers.0.self_attn.k_proj.bias\"{...})\n",
       "             41 |  # node_Concat_34\n",
       "                   %\"val_36\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_16\"{[-1]}, %\"val_34\"{[4]}, %\"val_35\"{[64]}) {axis=0}\n",
       "             42 |  # node_view_2\n",
       "                   %\"view_2\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_1\", %\"val_36\") {allowzero=1}\n",
       "             43 |  # node_transpose_2\n",
       "                   %\"transpose_2\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_2\") {perm=(0, 2, 1, 3)}\n",
       "             44 |  # node_MatMul_36\n",
       "                   %\"val_38\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_37\"{...})\n",
       "             45 |  # node_linear_2\n",
       "                   %\"linear_2\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_38\", %\"layers.0.self_attn.v_proj.bias\"{...})\n",
       "             46 |  # node_view_3\n",
       "                   %\"view_3\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_2\", %\"val_36\") {allowzero=1}\n",
       "             47 |  # node_transpose_3\n",
       "                   %\"transpose_3\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_3\") {perm=(0, 2, 1, 3)}\n",
       "             48 |  # node_mul_119\n",
       "                   %\"mul_119\"<INT64,[]> ⬅️ ::Mul(%\"sym_size_int_103\", %\"val_3\"{4})\n",
       "             49 |  # node_Concat_48\n",
       "                   %\"val_50\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_9\", %\"val_34\"{[4]}, %\"val_35\"{[64]}) {axis=0}\n",
       "             50 |  # node_view_4\n",
       "                   %\"view_4\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_81\", %\"val_50\") {allowzero=1}\n",
       "             51 |  # node_transpose_4\n",
       "                   %\"transpose_4\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_4\") {perm=(0, 2, 1, 3)}\n",
       "             52 |  # node_Reshape_50\n",
       "                   %\"val_52\"<INT64,[1]> ⬅️ ::Reshape(%\"mul_119\", %\"val_16\"{[-1]}) {allowzero=0}\n",
       "             53 |  # node_Concat_53\n",
       "                   %\"val_55\"<INT64,[3]> ⬅️ ::Concat(%\"val_52\", %\"val_16\"{[-1]}, %\"val_35\"{[64]}) {axis=0}\n",
       "             54 |  # node_view_5\n",
       "                   %\"view_5\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_4\", %\"val_55\") {allowzero=1}\n",
       "             55 |  # node_view_6\n",
       "                   %\"view_6\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_2\", %\"val_55\") {allowzero=1}\n",
       "             56 |  # node_view_7\n",
       "                   %\"view_7\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_3\", %\"val_55\") {allowzero=1}\n",
       "             57 |  # node_transpose_5\n",
       "                   %\"transpose_5\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_6\") {perm=(0, 2, 1)}\n",
       "             58 |  # node_bmm\n",
       "                   %\"bmm\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_5\", %\"transpose_5\")\n",
       "             59 |  # node_softmax\n",
       "                   %\"softmax\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm\") {axis=-1}\n",
       "             60 |  # node_bmm_1\n",
       "                   %\"bmm_1\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax\", %\"view_7\")\n",
       "             61 |  # node_Concat_69\n",
       "                   %\"val_71\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_34\"{[4]}, %\"val_9\", %\"val_35\"{[64]}) {axis=0}\n",
       "             62 |  # node_view_8\n",
       "                   %\"view_8\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_1\", %\"val_71\") {allowzero=1}\n",
       "             63 |  # node_transpose_6\n",
       "                   %\"transpose_6\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_8\") {perm=(0, 2, 1, 3)}\n",
       "             64 |  # node_Concat_74\n",
       "                   %\"val_76\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_9\", %\"val_75\"{[256]}) {axis=0}\n",
       "             65 |  # node__unsafe_view\n",
       "                   %\"_unsafe_view\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_6\", %\"val_76\") {allowzero=1}\n",
       "             66 |  # node_MatMul_76\n",
       "                   %\"val_78\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view\", %\"val_77\"{...})\n",
       "             67 |  # node_linear_3\n",
       "                   %\"linear_3\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_78\", %\"layers.0.self_attn.out_proj.bias\"{...})\n",
       "             68 |  # node_add_213\n",
       "                   %\"add_213\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_97\", %\"linear_3\")\n",
       "             69 |  # node_layer_norm_1\n",
       "                   %\"layer_norm_1\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_213\", %\"layers.0.final_layer_norm.weight\"{...}, %\"layers.0.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "             70 |  # node_MatMul_78\n",
       "                   %\"val_82\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_1\", %\"val_81\"{...})\n",
       "             71 |  # node_linear_4\n",
       "                   %\"linear_4\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_82\", %\"layers.0.fc1.bias\"{...})\n",
       "             72 |  # node_relu\n",
       "                   %\"relu\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_4\")\n",
       "             73 |  # node_MatMul_80\n",
       "                   %\"val_84\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu\", %\"val_83\"{...})\n",
       "             74 |  # node_linear_5\n",
       "                   %\"linear_5\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_84\", %\"layers.0.fc2.bias\"{...})\n",
       "             75 |  # node_add_236\n",
       "                   %\"add_236\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_213\", %\"linear_5\")\n",
       "             76 |  # node_layer_norm_2\n",
       "                   %\"layer_norm_2\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_236\", %\"layers.1.self_attn_layer_norm.weight\"{...}, %\"layers.1.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "             77 |  # node_MatMul_82\n",
       "                   %\"val_88\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_87\"{...})\n",
       "             78 |  # node_linear_6\n",
       "                   %\"linear_6\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_88\", %\"layers.1.self_attn.q_proj.bias\"{...})\n",
       "             79 |  # node_mul_245\n",
       "                   %\"mul_245\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_6\", %\"val_28\"{0.125})\n",
       "             80 |  # node_MatMul_84\n",
       "                   %\"val_90\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_89\"{...})\n",
       "             81 |  # node_linear_7\n",
       "                   %\"linear_7\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_90\", %\"layers.1.self_attn.k_proj.bias\"{...})\n",
       "             82 |  # node_view_9\n",
       "                   %\"view_9\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_7\", %\"val_36\") {allowzero=1}\n",
       "             83 |  # node_transpose_7\n",
       "                   %\"transpose_7\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_9\") {perm=(0, 2, 1, 3)}\n",
       "             84 |  # node_MatMul_92\n",
       "                   %\"val_98\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_97\"{...})\n",
       "             85 |  # node_linear_8\n",
       "                   %\"linear_8\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_98\", %\"layers.1.self_attn.v_proj.bias\"{...})\n",
       "             86 |  # node_view_10\n",
       "                   %\"view_10\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_8\", %\"val_36\") {allowzero=1}\n",
       "             87 |  # node_transpose_8\n",
       "                   %\"transpose_8\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_10\") {perm=(0, 2, 1, 3)}\n",
       "             88 |  # node_view_11\n",
       "                   %\"view_11\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_245\", %\"val_50\") {allowzero=1}\n",
       "             89 |  # node_transpose_9\n",
       "                   %\"transpose_9\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_11\") {perm=(0, 2, 1, 3)}\n",
       "             90 |  # node_view_12\n",
       "                   %\"view_12\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_9\", %\"val_55\") {allowzero=1}\n",
       "             91 |  # node_view_13\n",
       "                   %\"view_13\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_7\", %\"val_55\") {allowzero=1}\n",
       "             92 |  # node_view_14\n",
       "                   %\"view_14\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_8\", %\"val_55\") {allowzero=1}\n",
       "             93 |  # node_transpose_10\n",
       "                   %\"transpose_10\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_13\") {perm=(0, 2, 1)}\n",
       "             94 |  # node_bmm_2\n",
       "                   %\"bmm_2\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_12\", %\"transpose_10\")\n",
       "             95 |  # node_softmax_1\n",
       "                   %\"softmax_1\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_2\") {axis=-1}\n",
       "             96 |  # node_bmm_3\n",
       "                   %\"bmm_3\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_1\", %\"view_14\")\n",
       "             97 |  # node_view_15\n",
       "                   %\"view_15\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_3\", %\"val_71\") {allowzero=1}\n",
       "             98 |  # node_transpose_11\n",
       "                   %\"transpose_11\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_15\") {perm=(0, 2, 1, 3)}\n",
       "             99 |  # node__unsafe_view_1\n",
       "                   %\"_unsafe_view_1\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_11\", %\"val_76\") {allowzero=1}\n",
       "            100 |  # node_MatMul_132\n",
       "                   %\"val_138\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_1\", %\"val_137\"{...})\n",
       "            101 |  # node_linear_9\n",
       "                   %\"linear_9\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_138\", %\"layers.1.self_attn.out_proj.bias\"{...})\n",
       "            102 |  # node_add_347\n",
       "                   %\"add_347\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_236\", %\"linear_9\")\n",
       "            103 |  # node_layer_norm_3\n",
       "                   %\"layer_norm_3\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_347\", %\"layers.1.final_layer_norm.weight\"{...}, %\"layers.1.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            104 |  # node_MatMul_134\n",
       "                   %\"val_142\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_3\", %\"val_141\"{...})\n",
       "            105 |  # node_linear_10\n",
       "                   %\"linear_10\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_142\", %\"layers.1.fc1.bias\"{...})\n",
       "            106 |  # node_relu_1\n",
       "                   %\"relu_1\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_10\")\n",
       "            107 |  # node_MatMul_136\n",
       "                   %\"val_144\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_1\", %\"val_143\"{...})\n",
       "            108 |  # node_linear_11\n",
       "                   %\"linear_11\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_144\", %\"layers.1.fc2.bias\"{...})\n",
       "            109 |  # node_add_370\n",
       "                   %\"add_370\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_347\", %\"linear_11\")\n",
       "            110 |  # node_layer_norm_4\n",
       "                   %\"layer_norm_4\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_370\", %\"layers.2.self_attn_layer_norm.weight\"{...}, %\"layers.2.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            111 |  # node_MatMul_138\n",
       "                   %\"val_148\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_147\"{...})\n",
       "            112 |  # node_linear_12\n",
       "                   %\"linear_12\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_148\", %\"layers.2.self_attn.q_proj.bias\"{...})\n",
       "            113 |  # node_mul_408\n",
       "                   %\"mul_408\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_12\", %\"val_28\"{0.125})\n",
       "            114 |  # node_MatMul_140\n",
       "                   %\"val_150\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_149\"{...})\n",
       "            115 |  # node_linear_13\n",
       "                   %\"linear_13\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_150\", %\"layers.2.self_attn.k_proj.bias\"{...})\n",
       "            116 |  # node_view_16\n",
       "                   %\"view_16\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_13\", %\"val_36\") {allowzero=1}\n",
       "            117 |  # node_transpose_12\n",
       "                   %\"transpose_12\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_16\") {perm=(0, 2, 1, 3)}\n",
       "            118 |  # node_MatMul_148\n",
       "                   %\"val_158\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_4\", %\"val_157\"{...})\n",
       "            119 |  # node_linear_14\n",
       "                   %\"linear_14\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_158\", %\"layers.2.self_attn.v_proj.bias\"{...})\n",
       "            120 |  # node_view_17\n",
       "                   %\"view_17\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_14\", %\"val_36\") {allowzero=1}\n",
       "            121 |  # node_transpose_13\n",
       "                   %\"transpose_13\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_17\") {perm=(0, 2, 1, 3)}\n",
       "            122 |  # node_view_18\n",
       "                   %\"view_18\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_408\", %\"val_50\") {allowzero=1}\n",
       "            123 |  # node_transpose_14\n",
       "                   %\"transpose_14\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_18\") {perm=(0, 2, 1, 3)}\n",
       "            124 |  # node_view_19\n",
       "                   %\"view_19\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_14\", %\"val_55\") {allowzero=1}\n",
       "            125 |  # node_view_20\n",
       "                   %\"view_20\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_12\", %\"val_55\") {allowzero=1}\n",
       "            126 |  # node_view_21\n",
       "                   %\"view_21\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_13\", %\"val_55\") {allowzero=1}\n",
       "            127 |  # node_transpose_15\n",
       "                   %\"transpose_15\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_20\") {perm=(0, 2, 1)}\n",
       "            128 |  # node_bmm_4\n",
       "                   %\"bmm_4\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_19\", %\"transpose_15\")\n",
       "            129 |  # node_softmax_2\n",
       "                   %\"softmax_2\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_4\") {axis=-1}\n",
       "            130 |  # node_bmm_5\n",
       "                   %\"bmm_5\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_2\", %\"view_21\")\n",
       "            131 |  # node_view_22\n",
       "                   %\"view_22\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_5\", %\"val_71\") {allowzero=1}\n",
       "            132 |  # node_transpose_16\n",
       "                   %\"transpose_16\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_22\") {perm=(0, 2, 1, 3)}\n",
       "            133 |  # node__unsafe_view_2\n",
       "                   %\"_unsafe_view_2\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_16\", %\"val_76\") {allowzero=1}\n",
       "            134 |  # node_MatMul_188\n",
       "                   %\"val_198\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_2\", %\"val_197\"{...})\n",
       "            135 |  # node_linear_15\n",
       "                   %\"linear_15\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_198\", %\"layers.2.self_attn.out_proj.bias\"{...})\n",
       "            136 |  # node_add_481\n",
       "                   %\"add_481\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_370\", %\"linear_15\")\n",
       "            137 |  # node_layer_norm_5\n",
       "                   %\"layer_norm_5\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_481\", %\"layers.2.final_layer_norm.weight\"{...}, %\"layers.2.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            138 |  # node_MatMul_190\n",
       "                   %\"val_202\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_5\", %\"val_201\"{...})\n",
       "            139 |  # node_linear_16\n",
       "                   %\"linear_16\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_202\", %\"layers.2.fc1.bias\"{...})\n",
       "            140 |  # node_relu_2\n",
       "                   %\"relu_2\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_16\")\n",
       "            141 |  # node_MatMul_192\n",
       "                   %\"val_204\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_2\", %\"val_203\"{...})\n",
       "            142 |  # node_linear_17\n",
       "                   %\"linear_17\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_204\", %\"layers.2.fc2.bias\"{...})\n",
       "            143 |  # node_add_504\n",
       "                   %\"add_504\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_481\", %\"linear_17\")\n",
       "            144 |  # node_layer_norm_6\n",
       "                   %\"layer_norm_6\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_504\", %\"layers.3.self_attn_layer_norm.weight\"{...}, %\"layers.3.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            145 |  # node_MatMul_194\n",
       "                   %\"val_208\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_207\"{...})\n",
       "            146 |  # node_linear_18\n",
       "                   %\"linear_18\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_208\", %\"layers.3.self_attn.q_proj.bias\"{...})\n",
       "            147 |  # node_mul_571\n",
       "                   %\"mul_571\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_18\", %\"val_28\"{0.125})\n",
       "            148 |  # node_MatMul_196\n",
       "                   %\"val_210\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_209\"{...})\n",
       "            149 |  # node_linear_19\n",
       "                   %\"linear_19\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_210\", %\"layers.3.self_attn.k_proj.bias\"{...})\n",
       "            150 |  # node_view_23\n",
       "                   %\"view_23\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_19\", %\"val_36\") {allowzero=1}\n",
       "            151 |  # node_transpose_17\n",
       "                   %\"transpose_17\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_23\") {perm=(0, 2, 1, 3)}\n",
       "            152 |  # node_MatMul_204\n",
       "                   %\"val_218\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_6\", %\"val_217\"{...})\n",
       "            153 |  # node_linear_20\n",
       "                   %\"linear_20\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_218\", %\"layers.3.self_attn.v_proj.bias\"{...})\n",
       "            154 |  # node_view_24\n",
       "                   %\"view_24\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_20\", %\"val_36\") {allowzero=1}\n",
       "            155 |  # node_transpose_18\n",
       "                   %\"transpose_18\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_24\") {perm=(0, 2, 1, 3)}\n",
       "            156 |  # node_view_25\n",
       "                   %\"view_25\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_571\", %\"val_50\") {allowzero=1}\n",
       "            157 |  # node_transpose_19\n",
       "                   %\"transpose_19\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_25\") {perm=(0, 2, 1, 3)}\n",
       "            158 |  # node_view_26\n",
       "                   %\"view_26\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_19\", %\"val_55\") {allowzero=1}\n",
       "            159 |  # node_view_27\n",
       "                   %\"view_27\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_17\", %\"val_55\") {allowzero=1}\n",
       "            160 |  # node_view_28\n",
       "                   %\"view_28\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_18\", %\"val_55\") {allowzero=1}\n",
       "            161 |  # node_transpose_20\n",
       "                   %\"transpose_20\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_27\") {perm=(0, 2, 1)}\n",
       "            162 |  # node_bmm_6\n",
       "                   %\"bmm_6\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_26\", %\"transpose_20\")\n",
       "            163 |  # node_softmax_3\n",
       "                   %\"softmax_3\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_6\") {axis=-1}\n",
       "            164 |  # node_bmm_7\n",
       "                   %\"bmm_7\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_3\", %\"view_28\")\n",
       "            165 |  # node_view_29\n",
       "                   %\"view_29\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_7\", %\"val_71\") {allowzero=1}\n",
       "            166 |  # node_transpose_21\n",
       "                   %\"transpose_21\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_29\") {perm=(0, 2, 1, 3)}\n",
       "            167 |  # node__unsafe_view_3\n",
       "                   %\"_unsafe_view_3\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_21\", %\"val_76\") {allowzero=1}\n",
       "            168 |  # node_MatMul_244\n",
       "                   %\"val_258\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_3\", %\"val_257\"{...})\n",
       "            169 |  # node_linear_21\n",
       "                   %\"linear_21\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_258\", %\"layers.3.self_attn.out_proj.bias\"{...})\n",
       "            170 |  # node_add_615\n",
       "                   %\"add_615\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_504\", %\"linear_21\")\n",
       "            171 |  # node_layer_norm_7\n",
       "                   %\"layer_norm_7\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_615\", %\"layers.3.final_layer_norm.weight\"{...}, %\"layers.3.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            172 |  # node_MatMul_246\n",
       "                   %\"val_262\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_7\", %\"val_261\"{...})\n",
       "            173 |  # node_linear_22\n",
       "                   %\"linear_22\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_262\", %\"layers.3.fc1.bias\"{...})\n",
       "            174 |  # node_relu_3\n",
       "                   %\"relu_3\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_22\")\n",
       "            175 |  # node_MatMul_248\n",
       "                   %\"val_264\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_3\", %\"val_263\"{...})\n",
       "            176 |  # node_linear_23\n",
       "                   %\"linear_23\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_264\", %\"layers.3.fc2.bias\"{...})\n",
       "            177 |  # node_add_638\n",
       "                   %\"add_638\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_615\", %\"linear_23\")\n",
       "            178 |  # node_layer_norm_8\n",
       "                   %\"layer_norm_8\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_638\", %\"layers.4.self_attn_layer_norm.weight\"{...}, %\"layers.4.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            179 |  # node_MatMul_250\n",
       "                   %\"val_268\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_267\"{...})\n",
       "            180 |  # node_linear_24\n",
       "                   %\"linear_24\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_268\", %\"layers.4.self_attn.q_proj.bias\"{...})\n",
       "            181 |  # node_mul_734\n",
       "                   %\"mul_734\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_24\", %\"val_28\"{0.125})\n",
       "            182 |  # node_MatMul_252\n",
       "                   %\"val_270\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_269\"{...})\n",
       "            183 |  # node_linear_25\n",
       "                   %\"linear_25\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_270\", %\"layers.4.self_attn.k_proj.bias\"{...})\n",
       "            184 |  # node_view_30\n",
       "                   %\"view_30\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_25\", %\"val_36\") {allowzero=1}\n",
       "            185 |  # node_transpose_22\n",
       "                   %\"transpose_22\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_30\") {perm=(0, 2, 1, 3)}\n",
       "            186 |  # node_MatMul_260\n",
       "                   %\"val_278\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_8\", %\"val_277\"{...})\n",
       "            187 |  # node_linear_26\n",
       "                   %\"linear_26\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_278\", %\"layers.4.self_attn.v_proj.bias\"{...})\n",
       "            188 |  # node_view_31\n",
       "                   %\"view_31\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_26\", %\"val_36\") {allowzero=1}\n",
       "            189 |  # node_transpose_23\n",
       "                   %\"transpose_23\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_31\") {perm=(0, 2, 1, 3)}\n",
       "            190 |  # node_view_32\n",
       "                   %\"view_32\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_734\", %\"val_50\") {allowzero=1}\n",
       "            191 |  # node_transpose_24\n",
       "                   %\"transpose_24\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_32\") {perm=(0, 2, 1, 3)}\n",
       "            192 |  # node_view_33\n",
       "                   %\"view_33\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_24\", %\"val_55\") {allowzero=1}\n",
       "            193 |  # node_view_34\n",
       "                   %\"view_34\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_22\", %\"val_55\") {allowzero=1}\n",
       "            194 |  # node_view_35\n",
       "                   %\"view_35\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_23\", %\"val_55\") {allowzero=1}\n",
       "            195 |  # node_transpose_25\n",
       "                   %\"transpose_25\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_34\") {perm=(0, 2, 1)}\n",
       "            196 |  # node_bmm_8\n",
       "                   %\"bmm_8\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_33\", %\"transpose_25\")\n",
       "            197 |  # node_softmax_4\n",
       "                   %\"softmax_4\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_8\") {axis=-1}\n",
       "            198 |  # node_bmm_9\n",
       "                   %\"bmm_9\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_4\", %\"view_35\")\n",
       "            199 |  # node_view_36\n",
       "                   %\"view_36\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_9\", %\"val_71\") {allowzero=1}\n",
       "            200 |  # node_transpose_26\n",
       "                   %\"transpose_26\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_36\") {perm=(0, 2, 1, 3)}\n",
       "            201 |  # node__unsafe_view_4\n",
       "                   %\"_unsafe_view_4\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_26\", %\"val_76\") {allowzero=1}\n",
       "            202 |  # node_MatMul_300\n",
       "                   %\"val_318\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_4\", %\"val_317\"{...})\n",
       "            203 |  # node_linear_27\n",
       "                   %\"linear_27\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_318\", %\"layers.4.self_attn.out_proj.bias\"{...})\n",
       "            204 |  # node_add_749\n",
       "                   %\"add_749\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_638\", %\"linear_27\")\n",
       "            205 |  # node_layer_norm_9\n",
       "                   %\"layer_norm_9\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_749\", %\"layers.4.final_layer_norm.weight\"{...}, %\"layers.4.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            206 |  # node_MatMul_302\n",
       "                   %\"val_322\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_9\", %\"val_321\"{...})\n",
       "            207 |  # node_linear_28\n",
       "                   %\"linear_28\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_322\", %\"layers.4.fc1.bias\"{...})\n",
       "            208 |  # node_relu_4\n",
       "                   %\"relu_4\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_28\")\n",
       "            209 |  # node_MatMul_304\n",
       "                   %\"val_324\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_4\", %\"val_323\"{...})\n",
       "            210 |  # node_linear_29\n",
       "                   %\"linear_29\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_324\", %\"layers.4.fc2.bias\"{...})\n",
       "            211 |  # node_add_772\n",
       "                   %\"add_772\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_749\", %\"linear_29\")\n",
       "            212 |  # node_layer_norm_10\n",
       "                   %\"layer_norm_10\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_772\", %\"layers.5.self_attn_layer_norm.weight\"{...}, %\"layers.5.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            213 |  # node_MatMul_306\n",
       "                   %\"val_328\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_327\"{...})\n",
       "            214 |  # node_linear_30\n",
       "                   %\"linear_30\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_328\", %\"layers.5.self_attn.q_proj.bias\"{...})\n",
       "            215 |  # node_mul_897\n",
       "                   %\"mul_897\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_30\", %\"val_28\"{0.125})\n",
       "            216 |  # node_MatMul_308\n",
       "                   %\"val_330\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_329\"{...})\n",
       "            217 |  # node_linear_31\n",
       "                   %\"linear_31\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_330\", %\"layers.5.self_attn.k_proj.bias\"{...})\n",
       "            218 |  # node_view_37\n",
       "                   %\"view_37\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_31\", %\"val_36\") {allowzero=1}\n",
       "            219 |  # node_transpose_27\n",
       "                   %\"transpose_27\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_37\") {perm=(0, 2, 1, 3)}\n",
       "            220 |  # node_MatMul_316\n",
       "                   %\"val_338\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_10\", %\"val_337\"{...})\n",
       "            221 |  # node_linear_32\n",
       "                   %\"linear_32\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_338\", %\"layers.5.self_attn.v_proj.bias\"{...})\n",
       "            222 |  # node_view_38\n",
       "                   %\"view_38\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_32\", %\"val_36\") {allowzero=1}\n",
       "            223 |  # node_transpose_28\n",
       "                   %\"transpose_28\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_38\") {perm=(0, 2, 1, 3)}\n",
       "            224 |  # node_view_39\n",
       "                   %\"view_39\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_897\", %\"val_50\") {allowzero=1}\n",
       "            225 |  # node_transpose_29\n",
       "                   %\"transpose_29\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_39\") {perm=(0, 2, 1, 3)}\n",
       "            226 |  # node_view_40\n",
       "                   %\"view_40\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_29\", %\"val_55\") {allowzero=1}\n",
       "            227 |  # node_view_41\n",
       "                   %\"view_41\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_27\", %\"val_55\") {allowzero=1}\n",
       "            228 |  # node_view_42\n",
       "                   %\"view_42\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_28\", %\"val_55\") {allowzero=1}\n",
       "            229 |  # node_transpose_30\n",
       "                   %\"transpose_30\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_41\") {perm=(0, 2, 1)}\n",
       "            230 |  # node_bmm_10\n",
       "                   %\"bmm_10\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_40\", %\"transpose_30\")\n",
       "            231 |  # node_softmax_5\n",
       "                   %\"softmax_5\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_10\") {axis=-1}\n",
       "            232 |  # node_bmm_11\n",
       "                   %\"bmm_11\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_5\", %\"view_42\")\n",
       "            233 |  # node_view_43\n",
       "                   %\"view_43\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_11\", %\"val_71\") {allowzero=1}\n",
       "            234 |  # node_transpose_31\n",
       "                   %\"transpose_31\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_43\") {perm=(0, 2, 1, 3)}\n",
       "            235 |  # node__unsafe_view_5\n",
       "                   %\"_unsafe_view_5\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_31\", %\"val_76\") {allowzero=1}\n",
       "            236 |  # node_MatMul_356\n",
       "                   %\"val_378\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_5\", %\"val_377\"{...})\n",
       "            237 |  # node_linear_33\n",
       "                   %\"linear_33\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_378\", %\"layers.5.self_attn.out_proj.bias\"{...})\n",
       "            238 |  # node_add_883\n",
       "                   %\"add_883\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_772\", %\"linear_33\")\n",
       "            239 |  # node_layer_norm_11\n",
       "                   %\"layer_norm_11\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_883\", %\"layers.5.final_layer_norm.weight\"{...}, %\"layers.5.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            240 |  # node_MatMul_358\n",
       "                   %\"val_382\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_11\", %\"val_381\"{...})\n",
       "            241 |  # node_linear_34\n",
       "                   %\"linear_34\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_382\", %\"layers.5.fc1.bias\"{...})\n",
       "            242 |  # node_relu_5\n",
       "                   %\"relu_5\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_34\")\n",
       "            243 |  # node_MatMul_360\n",
       "                   %\"val_384\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_5\", %\"val_383\"{...})\n",
       "            244 |  # node_linear_35\n",
       "                   %\"linear_35\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_384\", %\"layers.5.fc2.bias\"{...})\n",
       "            245 |  # node_add_906\n",
       "                   %\"add_906\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_883\", %\"linear_35\")\n",
       "            246 |  # node_layer_norm_12\n",
       "                   %\"layer_norm_12\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_906\", %\"layers.6.self_attn_layer_norm.weight\"{...}, %\"layers.6.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            247 |  # node_MatMul_362\n",
       "                   %\"val_388\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_387\"{...})\n",
       "            248 |  # node_linear_36\n",
       "                   %\"linear_36\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_388\", %\"layers.6.self_attn.q_proj.bias\"{...})\n",
       "            249 |  # node_mul_1060\n",
       "                   %\"mul_1060\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_36\", %\"val_28\"{0.125})\n",
       "            250 |  # node_MatMul_364\n",
       "                   %\"val_390\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_389\"{...})\n",
       "            251 |  # node_linear_37\n",
       "                   %\"linear_37\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_390\", %\"layers.6.self_attn.k_proj.bias\"{...})\n",
       "            252 |  # node_view_44\n",
       "                   %\"view_44\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_37\", %\"val_36\") {allowzero=1}\n",
       "            253 |  # node_transpose_32\n",
       "                   %\"transpose_32\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_44\") {perm=(0, 2, 1, 3)}\n",
       "            254 |  # node_MatMul_372\n",
       "                   %\"val_398\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_12\", %\"val_397\"{...})\n",
       "            255 |  # node_linear_38\n",
       "                   %\"linear_38\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_398\", %\"layers.6.self_attn.v_proj.bias\"{...})\n",
       "            256 |  # node_view_45\n",
       "                   %\"view_45\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_38\", %\"val_36\") {allowzero=1}\n",
       "            257 |  # node_transpose_33\n",
       "                   %\"transpose_33\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_45\") {perm=(0, 2, 1, 3)}\n",
       "            258 |  # node_view_46\n",
       "                   %\"view_46\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1060\", %\"val_50\") {allowzero=1}\n",
       "            259 |  # node_transpose_34\n",
       "                   %\"transpose_34\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_46\") {perm=(0, 2, 1, 3)}\n",
       "            260 |  # node_view_47\n",
       "                   %\"view_47\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_34\", %\"val_55\") {allowzero=1}\n",
       "            261 |  # node_view_48\n",
       "                   %\"view_48\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_32\", %\"val_55\") {allowzero=1}\n",
       "            262 |  # node_view_49\n",
       "                   %\"view_49\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_33\", %\"val_55\") {allowzero=1}\n",
       "            263 |  # node_transpose_35\n",
       "                   %\"transpose_35\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_48\") {perm=(0, 2, 1)}\n",
       "            264 |  # node_bmm_12\n",
       "                   %\"bmm_12\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_47\", %\"transpose_35\")\n",
       "            265 |  # node_softmax_6\n",
       "                   %\"softmax_6\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_12\") {axis=-1}\n",
       "            266 |  # node_bmm_13\n",
       "                   %\"bmm_13\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_6\", %\"view_49\")\n",
       "            267 |  # node_view_50\n",
       "                   %\"view_50\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_13\", %\"val_71\") {allowzero=1}\n",
       "            268 |  # node_transpose_36\n",
       "                   %\"transpose_36\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_50\") {perm=(0, 2, 1, 3)}\n",
       "            269 |  # node__unsafe_view_6\n",
       "                   %\"_unsafe_view_6\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_36\", %\"val_76\") {allowzero=1}\n",
       "            270 |  # node_MatMul_412\n",
       "                   %\"val_438\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_6\", %\"val_437\"{...})\n",
       "            271 |  # node_linear_39\n",
       "                   %\"linear_39\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_438\", %\"layers.6.self_attn.out_proj.bias\"{...})\n",
       "            272 |  # node_add_1017\n",
       "                   %\"add_1017\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_906\", %\"linear_39\")\n",
       "            273 |  # node_layer_norm_13\n",
       "                   %\"layer_norm_13\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1017\", %\"layers.6.final_layer_norm.weight\"{...}, %\"layers.6.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            274 |  # node_MatMul_414\n",
       "                   %\"val_442\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_13\", %\"val_441\"{...})\n",
       "            275 |  # node_linear_40\n",
       "                   %\"linear_40\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_442\", %\"layers.6.fc1.bias\"{...})\n",
       "            276 |  # node_relu_6\n",
       "                   %\"relu_6\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_40\")\n",
       "            277 |  # node_MatMul_416\n",
       "                   %\"val_444\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_6\", %\"val_443\"{...})\n",
       "            278 |  # node_linear_41\n",
       "                   %\"linear_41\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_444\", %\"layers.6.fc2.bias\"{...})\n",
       "            279 |  # node_add_1040\n",
       "                   %\"add_1040\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1017\", %\"linear_41\")\n",
       "            280 |  # node_layer_norm_14\n",
       "                   %\"layer_norm_14\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1040\", %\"layers.7.self_attn_layer_norm.weight\"{...}, %\"layers.7.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            281 |  # node_MatMul_418\n",
       "                   %\"val_448\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_447\"{...})\n",
       "            282 |  # node_linear_42\n",
       "                   %\"linear_42\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_448\", %\"layers.7.self_attn.q_proj.bias\"{...})\n",
       "            283 |  # node_mul_1223\n",
       "                   %\"mul_1223\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_42\", %\"val_28\"{0.125})\n",
       "            284 |  # node_MatMul_420\n",
       "                   %\"val_450\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_449\"{...})\n",
       "            285 |  # node_linear_43\n",
       "                   %\"linear_43\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_450\", %\"layers.7.self_attn.k_proj.bias\"{...})\n",
       "            286 |  # node_view_51\n",
       "                   %\"view_51\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_43\", %\"val_36\") {allowzero=1}\n",
       "            287 |  # node_transpose_37\n",
       "                   %\"transpose_37\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_51\") {perm=(0, 2, 1, 3)}\n",
       "            288 |  # node_MatMul_428\n",
       "                   %\"val_458\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_14\", %\"val_457\"{...})\n",
       "            289 |  # node_linear_44\n",
       "                   %\"linear_44\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_458\", %\"layers.7.self_attn.v_proj.bias\"{...})\n",
       "            290 |  # node_view_52\n",
       "                   %\"view_52\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_44\", %\"val_36\") {allowzero=1}\n",
       "            291 |  # node_transpose_38\n",
       "                   %\"transpose_38\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_52\") {perm=(0, 2, 1, 3)}\n",
       "            292 |  # node_view_53\n",
       "                   %\"view_53\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1223\", %\"val_50\") {allowzero=1}\n",
       "            293 |  # node_transpose_39\n",
       "                   %\"transpose_39\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_53\") {perm=(0, 2, 1, 3)}\n",
       "            294 |  # node_view_54\n",
       "                   %\"view_54\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_39\", %\"val_55\") {allowzero=1}\n",
       "            295 |  # node_view_55\n",
       "                   %\"view_55\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_37\", %\"val_55\") {allowzero=1}\n",
       "            296 |  # node_view_56\n",
       "                   %\"view_56\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_38\", %\"val_55\") {allowzero=1}\n",
       "            297 |  # node_transpose_40\n",
       "                   %\"transpose_40\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_55\") {perm=(0, 2, 1)}\n",
       "            298 |  # node_bmm_14\n",
       "                   %\"bmm_14\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_54\", %\"transpose_40\")\n",
       "            299 |  # node_softmax_7\n",
       "                   %\"softmax_7\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_14\") {axis=-1}\n",
       "            300 |  # node_bmm_15\n",
       "                   %\"bmm_15\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_7\", %\"view_56\")\n",
       "            301 |  # node_view_57\n",
       "                   %\"view_57\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_15\", %\"val_71\") {allowzero=1}\n",
       "            302 |  # node_transpose_41\n",
       "                   %\"transpose_41\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_57\") {perm=(0, 2, 1, 3)}\n",
       "            303 |  # node__unsafe_view_7\n",
       "                   %\"_unsafe_view_7\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_41\", %\"val_76\") {allowzero=1}\n",
       "            304 |  # node_MatMul_468\n",
       "                   %\"val_498\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_7\", %\"val_497\"{...})\n",
       "            305 |  # node_linear_45\n",
       "                   %\"linear_45\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_498\", %\"layers.7.self_attn.out_proj.bias\"{...})\n",
       "            306 |  # node_add_1151\n",
       "                   %\"add_1151\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1040\", %\"linear_45\")\n",
       "            307 |  # node_layer_norm_15\n",
       "                   %\"layer_norm_15\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1151\", %\"layers.7.final_layer_norm.weight\"{...}, %\"layers.7.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            308 |  # node_MatMul_470\n",
       "                   %\"val_502\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_15\", %\"val_501\"{...})\n",
       "            309 |  # node_linear_46\n",
       "                   %\"linear_46\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_502\", %\"layers.7.fc1.bias\"{...})\n",
       "            310 |  # node_relu_7\n",
       "                   %\"relu_7\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_46\")\n",
       "            311 |  # node_MatMul_472\n",
       "                   %\"val_504\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_7\", %\"val_503\"{...})\n",
       "            312 |  # node_linear_47\n",
       "                   %\"linear_47\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_504\", %\"layers.7.fc2.bias\"{...})\n",
       "            313 |  # node_add_1174\n",
       "                   %\"add_1174\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1151\", %\"linear_47\")\n",
       "            314 |  # node_layer_norm_16\n",
       "                   %\"layer_norm_16\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1174\", %\"layers.8.self_attn_layer_norm.weight\"{...}, %\"layers.8.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            315 |  # node_MatMul_474\n",
       "                   %\"val_508\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_507\"{...})\n",
       "            316 |  # node_linear_48\n",
       "                   %\"linear_48\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_508\", %\"layers.8.self_attn.q_proj.bias\"{...})\n",
       "            317 |  # node_mul_1386\n",
       "                   %\"mul_1386\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_48\", %\"val_28\"{0.125})\n",
       "            318 |  # node_MatMul_476\n",
       "                   %\"val_510\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_509\"{...})\n",
       "            319 |  # node_linear_49\n",
       "                   %\"linear_49\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_510\", %\"layers.8.self_attn.k_proj.bias\"{...})\n",
       "            320 |  # node_view_58\n",
       "                   %\"view_58\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_49\", %\"val_36\") {allowzero=1}\n",
       "            321 |  # node_transpose_42\n",
       "                   %\"transpose_42\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_58\") {perm=(0, 2, 1, 3)}\n",
       "            322 |  # node_MatMul_484\n",
       "                   %\"val_518\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_16\", %\"val_517\"{...})\n",
       "            323 |  # node_linear_50\n",
       "                   %\"linear_50\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_518\", %\"layers.8.self_attn.v_proj.bias\"{...})\n",
       "            324 |  # node_view_59\n",
       "                   %\"view_59\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_50\", %\"val_36\") {allowzero=1}\n",
       "            325 |  # node_transpose_43\n",
       "                   %\"transpose_43\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_59\") {perm=(0, 2, 1, 3)}\n",
       "            326 |  # node_view_60\n",
       "                   %\"view_60\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1386\", %\"val_50\") {allowzero=1}\n",
       "            327 |  # node_transpose_44\n",
       "                   %\"transpose_44\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_60\") {perm=(0, 2, 1, 3)}\n",
       "            328 |  # node_view_61\n",
       "                   %\"view_61\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_44\", %\"val_55\") {allowzero=1}\n",
       "            329 |  # node_view_62\n",
       "                   %\"view_62\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_42\", %\"val_55\") {allowzero=1}\n",
       "            330 |  # node_view_63\n",
       "                   %\"view_63\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_43\", %\"val_55\") {allowzero=1}\n",
       "            331 |  # node_transpose_45\n",
       "                   %\"transpose_45\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_62\") {perm=(0, 2, 1)}\n",
       "            332 |  # node_bmm_16\n",
       "                   %\"bmm_16\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_61\", %\"transpose_45\")\n",
       "            333 |  # node_softmax_8\n",
       "                   %\"softmax_8\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_16\") {axis=-1}\n",
       "            334 |  # node_bmm_17\n",
       "                   %\"bmm_17\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_8\", %\"view_63\")\n",
       "            335 |  # node_view_64\n",
       "                   %\"view_64\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_17\", %\"val_71\") {allowzero=1}\n",
       "            336 |  # node_transpose_46\n",
       "                   %\"transpose_46\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_64\") {perm=(0, 2, 1, 3)}\n",
       "            337 |  # node__unsafe_view_8\n",
       "                   %\"_unsafe_view_8\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_46\", %\"val_76\") {allowzero=1}\n",
       "            338 |  # node_MatMul_524\n",
       "                   %\"val_558\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_8\", %\"val_557\"{...})\n",
       "            339 |  # node_linear_51\n",
       "                   %\"linear_51\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_558\", %\"layers.8.self_attn.out_proj.bias\"{...})\n",
       "            340 |  # node_add_1285\n",
       "                   %\"add_1285\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1174\", %\"linear_51\")\n",
       "            341 |  # node_layer_norm_17\n",
       "                   %\"layer_norm_17\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1285\", %\"layers.8.final_layer_norm.weight\"{...}, %\"layers.8.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            342 |  # node_MatMul_526\n",
       "                   %\"val_562\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_17\", %\"val_561\"{...})\n",
       "            343 |  # node_linear_52\n",
       "                   %\"linear_52\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_562\", %\"layers.8.fc1.bias\"{...})\n",
       "            344 |  # node_relu_8\n",
       "                   %\"relu_8\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_52\")\n",
       "            345 |  # node_MatMul_528\n",
       "                   %\"val_564\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_8\", %\"val_563\"{...})\n",
       "            346 |  # node_linear_53\n",
       "                   %\"linear_53\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_564\", %\"layers.8.fc2.bias\"{...})\n",
       "            347 |  # node_add_1308\n",
       "                   %\"add_1308\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1285\", %\"linear_53\")\n",
       "            348 |  # node_layer_norm_18\n",
       "                   %\"layer_norm_18\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1308\", %\"layers.9.self_attn_layer_norm.weight\"{...}, %\"layers.9.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            349 |  # node_MatMul_530\n",
       "                   %\"val_568\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_567\"{...})\n",
       "            350 |  # node_linear_54\n",
       "                   %\"linear_54\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_568\", %\"layers.9.self_attn.q_proj.bias\"{...})\n",
       "            351 |  # node_mul_1549\n",
       "                   %\"mul_1549\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_54\", %\"val_28\"{0.125})\n",
       "            352 |  # node_MatMul_532\n",
       "                   %\"val_570\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_569\"{...})\n",
       "            353 |  # node_linear_55\n",
       "                   %\"linear_55\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_570\", %\"layers.9.self_attn.k_proj.bias\"{...})\n",
       "            354 |  # node_view_65\n",
       "                   %\"view_65\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_55\", %\"val_36\") {allowzero=1}\n",
       "            355 |  # node_transpose_47\n",
       "                   %\"transpose_47\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_65\") {perm=(0, 2, 1, 3)}\n",
       "            356 |  # node_MatMul_540\n",
       "                   %\"val_578\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_18\", %\"val_577\"{...})\n",
       "            357 |  # node_linear_56\n",
       "                   %\"linear_56\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_578\", %\"layers.9.self_attn.v_proj.bias\"{...})\n",
       "            358 |  # node_view_66\n",
       "                   %\"view_66\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_56\", %\"val_36\") {allowzero=1}\n",
       "            359 |  # node_transpose_48\n",
       "                   %\"transpose_48\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_66\") {perm=(0, 2, 1, 3)}\n",
       "            360 |  # node_view_67\n",
       "                   %\"view_67\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1549\", %\"val_50\") {allowzero=1}\n",
       "            361 |  # node_transpose_49\n",
       "                   %\"transpose_49\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_67\") {perm=(0, 2, 1, 3)}\n",
       "            362 |  # node_view_68\n",
       "                   %\"view_68\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_49\", %\"val_55\") {allowzero=1}\n",
       "            363 |  # node_view_69\n",
       "                   %\"view_69\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_47\", %\"val_55\") {allowzero=1}\n",
       "            364 |  # node_view_70\n",
       "                   %\"view_70\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_48\", %\"val_55\") {allowzero=1}\n",
       "            365 |  # node_transpose_50\n",
       "                   %\"transpose_50\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_69\") {perm=(0, 2, 1)}\n",
       "            366 |  # node_bmm_18\n",
       "                   %\"bmm_18\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_68\", %\"transpose_50\")\n",
       "            367 |  # node_softmax_9\n",
       "                   %\"softmax_9\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_18\") {axis=-1}\n",
       "            368 |  # node_bmm_19\n",
       "                   %\"bmm_19\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_9\", %\"view_70\")\n",
       "            369 |  # node_view_71\n",
       "                   %\"view_71\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_19\", %\"val_71\") {allowzero=1}\n",
       "            370 |  # node_transpose_51\n",
       "                   %\"transpose_51\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_71\") {perm=(0, 2, 1, 3)}\n",
       "            371 |  # node__unsafe_view_9\n",
       "                   %\"_unsafe_view_9\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_51\", %\"val_76\") {allowzero=1}\n",
       "            372 |  # node_MatMul_580\n",
       "                   %\"val_618\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_9\", %\"val_617\"{...})\n",
       "            373 |  # node_linear_57\n",
       "                   %\"linear_57\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_618\", %\"layers.9.self_attn.out_proj.bias\"{...})\n",
       "            374 |  # node_add_1419\n",
       "                   %\"add_1419\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1308\", %\"linear_57\")\n",
       "            375 |  # node_layer_norm_19\n",
       "                   %\"layer_norm_19\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1419\", %\"layers.9.final_layer_norm.weight\"{...}, %\"layers.9.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            376 |  # node_MatMul_582\n",
       "                   %\"val_622\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_19\", %\"val_621\"{...})\n",
       "            377 |  # node_linear_58\n",
       "                   %\"linear_58\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_622\", %\"layers.9.fc1.bias\"{...})\n",
       "            378 |  # node_relu_9\n",
       "                   %\"relu_9\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_58\")\n",
       "            379 |  # node_MatMul_584\n",
       "                   %\"val_624\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_9\", %\"val_623\"{...})\n",
       "            380 |  # node_linear_59\n",
       "                   %\"linear_59\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_624\", %\"layers.9.fc2.bias\"{...})\n",
       "            381 |  # node_add_1442\n",
       "                   %\"add_1442\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1419\", %\"linear_59\")\n",
       "            382 |  # node_layer_norm_20\n",
       "                   %\"layer_norm_20\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1442\", %\"layers.10.self_attn_layer_norm.weight\"{...}, %\"layers.10.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            383 |  # node_MatMul_586\n",
       "                   %\"val_628\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_627\"{...})\n",
       "            384 |  # node_linear_60\n",
       "                   %\"linear_60\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_628\", %\"layers.10.self_attn.q_proj.bias\"{...})\n",
       "            385 |  # node_mul_1712\n",
       "                   %\"mul_1712\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_60\", %\"val_28\"{0.125})\n",
       "            386 |  # node_MatMul_588\n",
       "                   %\"val_630\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_629\"{...})\n",
       "            387 |  # node_linear_61\n",
       "                   %\"linear_61\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_630\", %\"layers.10.self_attn.k_proj.bias\"{...})\n",
       "            388 |  # node_view_72\n",
       "                   %\"view_72\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_61\", %\"val_36\") {allowzero=1}\n",
       "            389 |  # node_transpose_52\n",
       "                   %\"transpose_52\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_72\") {perm=(0, 2, 1, 3)}\n",
       "            390 |  # node_MatMul_596\n",
       "                   %\"val_638\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_20\", %\"val_637\"{...})\n",
       "            391 |  # node_linear_62\n",
       "                   %\"linear_62\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_638\", %\"layers.10.self_attn.v_proj.bias\"{...})\n",
       "            392 |  # node_view_73\n",
       "                   %\"view_73\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_62\", %\"val_36\") {allowzero=1}\n",
       "            393 |  # node_transpose_53\n",
       "                   %\"transpose_53\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_73\") {perm=(0, 2, 1, 3)}\n",
       "            394 |  # node_view_74\n",
       "                   %\"view_74\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1712\", %\"val_50\") {allowzero=1}\n",
       "            395 |  # node_transpose_54\n",
       "                   %\"transpose_54\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_74\") {perm=(0, 2, 1, 3)}\n",
       "            396 |  # node_view_75\n",
       "                   %\"view_75\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_54\", %\"val_55\") {allowzero=1}\n",
       "            397 |  # node_view_76\n",
       "                   %\"view_76\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_52\", %\"val_55\") {allowzero=1}\n",
       "            398 |  # node_view_77\n",
       "                   %\"view_77\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_53\", %\"val_55\") {allowzero=1}\n",
       "            399 |  # node_transpose_55\n",
       "                   %\"transpose_55\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_76\") {perm=(0, 2, 1)}\n",
       "            400 |  # node_bmm_20\n",
       "                   %\"bmm_20\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_75\", %\"transpose_55\")\n",
       "            401 |  # node_softmax_10\n",
       "                   %\"softmax_10\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_20\") {axis=-1}\n",
       "            402 |  # node_bmm_21\n",
       "                   %\"bmm_21\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_10\", %\"view_77\")\n",
       "            403 |  # node_view_78\n",
       "                   %\"view_78\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_21\", %\"val_71\") {allowzero=1}\n",
       "            404 |  # node_transpose_56\n",
       "                   %\"transpose_56\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_78\") {perm=(0, 2, 1, 3)}\n",
       "            405 |  # node__unsafe_view_10\n",
       "                   %\"_unsafe_view_10\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_56\", %\"val_76\") {allowzero=1}\n",
       "            406 |  # node_MatMul_636\n",
       "                   %\"val_678\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_10\", %\"val_677\"{...})\n",
       "            407 |  # node_linear_63\n",
       "                   %\"linear_63\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_678\", %\"layers.10.self_attn.out_proj.bias\"{...})\n",
       "            408 |  # node_add_1553\n",
       "                   %\"add_1553\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1442\", %\"linear_63\")\n",
       "            409 |  # node_layer_norm_21\n",
       "                   %\"layer_norm_21\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1553\", %\"layers.10.final_layer_norm.weight\"{...}, %\"layers.10.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            410 |  # node_MatMul_638\n",
       "                   %\"val_682\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_21\", %\"val_681\"{...})\n",
       "            411 |  # node_linear_64\n",
       "                   %\"linear_64\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_682\", %\"layers.10.fc1.bias\"{...})\n",
       "            412 |  # node_relu_10\n",
       "                   %\"relu_10\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_64\")\n",
       "            413 |  # node_MatMul_640\n",
       "                   %\"val_684\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_10\", %\"val_683\"{...})\n",
       "            414 |  # node_linear_65\n",
       "                   %\"linear_65\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_684\", %\"layers.10.fc2.bias\"{...})\n",
       "            415 |  # node_add_1576\n",
       "                   %\"add_1576\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1553\", %\"linear_65\")\n",
       "            416 |  # node_layer_norm_22\n",
       "                   %\"layer_norm_22\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1576\", %\"layers.11.self_attn_layer_norm.weight\"{...}, %\"layers.11.self_attn_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            417 |  # node_MatMul_642\n",
       "                   %\"val_688\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_687\"{...})\n",
       "            418 |  # node_linear_66\n",
       "                   %\"linear_66\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_688\", %\"layers.11.self_attn.q_proj.bias\"{...})\n",
       "            419 |  # node_mul_1875\n",
       "                   %\"mul_1875\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Mul(%\"linear_66\", %\"val_28\"{0.125})\n",
       "            420 |  # node_MatMul_644\n",
       "                   %\"val_690\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_689\"{...})\n",
       "            421 |  # node_linear_67\n",
       "                   %\"linear_67\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_690\", %\"layers.11.self_attn.k_proj.bias\"{...})\n",
       "            422 |  # node_view_79\n",
       "                   %\"view_79\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_67\", %\"val_36\") {allowzero=1}\n",
       "            423 |  # node_transpose_57\n",
       "                   %\"transpose_57\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_79\") {perm=(0, 2, 1, 3)}\n",
       "            424 |  # node_MatMul_652\n",
       "                   %\"val_698\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"layer_norm_22\", %\"val_697\"{...})\n",
       "            425 |  # node_linear_68\n",
       "                   %\"linear_68\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_698\", %\"layers.11.self_attn.v_proj.bias\"{...})\n",
       "            426 |  # node_view_80\n",
       "                   %\"view_80\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"linear_68\", %\"val_36\") {allowzero=1}\n",
       "            427 |  # node_transpose_58\n",
       "                   %\"transpose_58\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_80\") {perm=(0, 2, 1, 3)}\n",
       "            428 |  # node_view_81\n",
       "                   %\"view_81\"<FLOAT,[1,(((s37 - 1)//4)) + 1,4,64]> ⬅️ ::Reshape(%\"mul_1875\", %\"val_50\") {allowzero=1}\n",
       "            429 |  # node_transpose_59\n",
       "                   %\"transpose_59\"<FLOAT,[1,4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Transpose(%\"view_81\") {perm=(0, 2, 1, 3)}\n",
       "            430 |  # node_view_82\n",
       "                   %\"view_82\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_59\", %\"val_55\") {allowzero=1}\n",
       "            431 |  # node_view_83\n",
       "                   %\"view_83\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_57\", %\"val_55\") {allowzero=1}\n",
       "            432 |  # node_view_84\n",
       "                   %\"view_84\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"transpose_58\", %\"val_55\") {allowzero=1}\n",
       "            433 |  # node_transpose_60\n",
       "                   %\"transpose_60\"<FLOAT,[4,64,(((s37 - 1)//4)) + 1]> ⬅️ ::Transpose(%\"view_83\") {perm=(0, 2, 1)}\n",
       "            434 |  # node_bmm_22\n",
       "                   %\"bmm_22\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::MatMul(%\"view_82\", %\"transpose_60\")\n",
       "            435 |  # node_softmax_11\n",
       "                   %\"softmax_11\"<FLOAT,[4,(((s37 - 1)//4)) + 1,(((s37 - 1)//4)) + 1]> ⬅️ ::Softmax(%\"bmm_22\") {axis=-1}\n",
       "            436 |  # node_bmm_23\n",
       "                   %\"bmm_23\"<FLOAT,[4,(((s37 - 1)//4)) + 1,64]> ⬅️ ::MatMul(%\"softmax_11\", %\"view_84\")\n",
       "            437 |  # node_view_85\n",
       "                   %\"view_85\"<FLOAT,[s6,(4//s6),(((s37 - 1)//4)) + 1,64]> ⬅️ ::Reshape(%\"bmm_23\", %\"val_71\") {allowzero=1}\n",
       "            438 |  # node_transpose_61\n",
       "                   %\"transpose_61\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,(4//s6),64]> ⬅️ ::Transpose(%\"view_85\") {perm=(0, 2, 1, 3)}\n",
       "            439 |  # node__unsafe_view_11\n",
       "                   %\"_unsafe_view_11\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,64*((4//s6))]> ⬅️ ::Reshape(%\"transpose_61\", %\"val_76\") {allowzero=1}\n",
       "            440 |  # node_MatMul_692\n",
       "                   %\"val_738\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"_unsafe_view_11\", %\"val_737\"{...})\n",
       "            441 |  # node_linear_69\n",
       "                   %\"linear_69\"<FLOAT,[s6,(((((s37 - 1)//4)) + 1)//s6),256]> ⬅️ ::Add(%\"val_738\", %\"layers.11.self_attn.out_proj.bias\"{...})\n",
       "            442 |  # node_add_1687\n",
       "                   %\"add_1687\"<FLOAT,[s6,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1576\", %\"linear_69\")\n",
       "            443 |  # node_layer_norm_23\n",
       "                   %\"layer_norm_23\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1687\", %\"layers.11.final_layer_norm.weight\"{...}, %\"layers.11.final_layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            444 |  # node_MatMul_694\n",
       "                   %\"val_742\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::MatMul(%\"layer_norm_23\", %\"val_741\"{...})\n",
       "            445 |  # node_linear_70\n",
       "                   %\"linear_70\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Add(%\"val_742\", %\"layers.11.fc1.bias\"{...})\n",
       "            446 |  # node_relu_11\n",
       "                   %\"relu_11\"<FLOAT,[1,(((s37 - 1)//4)) + 1,2048]> ⬅️ ::Relu(%\"linear_70\")\n",
       "            447 |  # node_MatMul_696\n",
       "                   %\"val_744\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::MatMul(%\"relu_11\", %\"val_743\"{...})\n",
       "            448 |  # node_linear_71\n",
       "                   %\"linear_71\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"val_744\", %\"layers.11.fc2.bias\"{...})\n",
       "            449 |  # node_add_1710\n",
       "                   %\"add_1710\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]> ⬅️ ::Add(%\"add_1687\", %\"linear_71\")\n",
       "            450 |  # node_layer_norm_24\n",
       "                   %\"encoder_outputs\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_1710\", %\"layer_norm.weight\"{...}, %\"layer_norm.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            return %\"encoder_outputs\"<FLOAT,[1,(((s37 - 1)//4)) + 1,256]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_conv_conv_layers_0_weight: \"f32[1024, 80, 5]\", p_conv_conv_layers_0_bias: \"f32[1024]\", p_conv_conv_layers_1_weight: \"f32[512, 512, 5]\", p_conv_conv_layers_1_bias: \"f32[512]\", p_embed_positions_weights: \"f32[6002, 256]\", p_layers_0_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_0_self_attn_k_proj_bias: \"f32[256]\", p_layers_0_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_0_self_attn_v_proj_bias: \"f32[256]\", p_layers_0_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_0_self_attn_q_proj_bias: \"f32[256]\", p_layers_0_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_0_self_attn_out_proj_bias: \"f32[256]\", p_layers_0_self_attn_layer_norm_weight: \"f32[256]\", p_layers_0_self_attn_layer_norm_bias: \"f32[256]\", p_layers_0_fc1_weight: \"f32[2048, 256]\", p_layers_0_fc1_bias: \"f32[2048]\", p_layers_0_fc2_weight: \"f32[256, 2048]\", p_layers_0_fc2_bias: \"f32[256]\", p_layers_0_final_layer_norm_weight: \"f32[256]\", p_layers_0_final_layer_norm_bias: \"f32[256]\", p_layers_1_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_1_self_attn_k_proj_bias: \"f32[256]\", p_layers_1_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_1_self_attn_v_proj_bias: \"f32[256]\", p_layers_1_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_1_self_attn_q_proj_bias: \"f32[256]\", p_layers_1_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_1_self_attn_out_proj_bias: \"f32[256]\", p_layers_1_self_attn_layer_norm_weight: \"f32[256]\", p_layers_1_self_attn_layer_norm_bias: \"f32[256]\", p_layers_1_fc1_weight: \"f32[2048, 256]\", p_layers_1_fc1_bias: \"f32[2048]\", p_layers_1_fc2_weight: \"f32[256, 2048]\", p_layers_1_fc2_bias: \"f32[256]\", p_layers_1_final_layer_norm_weight: \"f32[256]\", p_layers_1_final_layer_norm_bias: \"f32[256]\", p_layers_2_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_2_self_attn_k_proj_bias: \"f32[256]\", p_layers_2_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_2_self_attn_v_proj_bias: \"f32[256]\", p_layers_2_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_2_self_attn_q_proj_bias: \"f32[256]\", p_layers_2_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_2_self_attn_out_proj_bias: \"f32[256]\", p_layers_2_self_attn_layer_norm_weight: \"f32[256]\", p_layers_2_self_attn_layer_norm_bias: \"f32[256]\", p_layers_2_fc1_weight: \"f32[2048, 256]\", p_layers_2_fc1_bias: \"f32[2048]\", p_layers_2_fc2_weight: \"f32[256, 2048]\", p_layers_2_fc2_bias: \"f32[256]\", p_layers_2_final_layer_norm_weight: \"f32[256]\", p_layers_2_final_layer_norm_bias: \"f32[256]\", p_layers_3_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_3_self_attn_k_proj_bias: \"f32[256]\", p_layers_3_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_3_self_attn_v_proj_bias: \"f32[256]\", p_layers_3_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_3_self_attn_q_proj_bias: \"f32[256]\", p_layers_3_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_3_self_attn_out_proj_bias: \"f32[256]\", p_layers_3_self_attn_layer_norm_weight: \"f32[256]\", p_layers_3_self_attn_layer_norm_bias: \"f32[256]\", p_layers_3_fc1_weight: \"f32[2048, 256]\", p_layers_3_fc1_bias: \"f32[2048]\", p_layers_3_fc2_weight: \"f32[256, 2048]\", p_layers_3_fc2_bias: \"f32[256]\", p_layers_3_final_layer_norm_weight: \"f32[256]\", p_layers_3_final_layer_norm_bias: \"f32[256]\", p_layers_4_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_4_self_attn_k_proj_bias: \"f32[256]\", p_layers_4_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_4_self_attn_v_proj_bias: \"f32[256]\", p_layers_4_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_4_self_attn_q_proj_bias: \"f32[256]\", p_layers_4_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_4_self_attn_out_proj_bias: \"f32[256]\", p_layers_4_self_attn_layer_norm_weight: \"f32[256]\", p_layers_4_self_attn_layer_norm_bias: \"f32[256]\", p_layers_4_fc1_weight: \"f32[2048, 256]\", p_layers_4_fc1_bias: \"f32[2048]\", p_layers_4_fc2_weight: \"f32[256, 2048]\", p_layers_4_fc2_bias: \"f32[256]\", p_layers_4_final_layer_norm_weight: \"f32[256]\", p_layers_4_final_layer_norm_bias: \"f32[256]\", p_layers_5_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_5_self_attn_k_proj_bias: \"f32[256]\", p_layers_5_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_5_self_attn_v_proj_bias: \"f32[256]\", p_layers_5_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_5_self_attn_q_proj_bias: \"f32[256]\", p_layers_5_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_5_self_attn_out_proj_bias: \"f32[256]\", p_layers_5_self_attn_layer_norm_weight: \"f32[256]\", p_layers_5_self_attn_layer_norm_bias: \"f32[256]\", p_layers_5_fc1_weight: \"f32[2048, 256]\", p_layers_5_fc1_bias: \"f32[2048]\", p_layers_5_fc2_weight: \"f32[256, 2048]\", p_layers_5_fc2_bias: \"f32[256]\", p_layers_5_final_layer_norm_weight: \"f32[256]\", p_layers_5_final_layer_norm_bias: \"f32[256]\", p_layers_6_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_6_self_attn_k_proj_bias: \"f32[256]\", p_layers_6_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_6_self_attn_v_proj_bias: \"f32[256]\", p_layers_6_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_6_self_attn_q_proj_bias: \"f32[256]\", p_layers_6_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_6_self_attn_out_proj_bias: \"f32[256]\", p_layers_6_self_attn_layer_norm_weight: \"f32[256]\", p_layers_6_self_attn_layer_norm_bias: \"f32[256]\", p_layers_6_fc1_weight: \"f32[2048, 256]\", p_layers_6_fc1_bias: \"f32[2048]\", p_layers_6_fc2_weight: \"f32[256, 2048]\", p_layers_6_fc2_bias: \"f32[256]\", p_layers_6_final_layer_norm_weight: \"f32[256]\", p_layers_6_final_layer_norm_bias: \"f32[256]\", p_layers_7_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_7_self_attn_k_proj_bias: \"f32[256]\", p_layers_7_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_7_self_attn_v_proj_bias: \"f32[256]\", p_layers_7_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_7_self_attn_q_proj_bias: \"f32[256]\", p_layers_7_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_7_self_attn_out_proj_bias: \"f32[256]\", p_layers_7_self_attn_layer_norm_weight: \"f32[256]\", p_layers_7_self_attn_layer_norm_bias: \"f32[256]\", p_layers_7_fc1_weight: \"f32[2048, 256]\", p_layers_7_fc1_bias: \"f32[2048]\", p_layers_7_fc2_weight: \"f32[256, 2048]\", p_layers_7_fc2_bias: \"f32[256]\", p_layers_7_final_layer_norm_weight: \"f32[256]\", p_layers_7_final_layer_norm_bias: \"f32[256]\", p_layers_8_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_8_self_attn_k_proj_bias: \"f32[256]\", p_layers_8_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_8_self_attn_v_proj_bias: \"f32[256]\", p_layers_8_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_8_self_attn_q_proj_bias: \"f32[256]\", p_layers_8_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_8_self_attn_out_proj_bias: \"f32[256]\", p_layers_8_self_attn_layer_norm_weight: \"f32[256]\", p_layers_8_self_attn_layer_norm_bias: \"f32[256]\", p_layers_8_fc1_weight: \"f32[2048, 256]\", p_layers_8_fc1_bias: \"f32[2048]\", p_layers_8_fc2_weight: \"f32[256, 2048]\", p_layers_8_fc2_bias: \"f32[256]\", p_layers_8_final_layer_norm_weight: \"f32[256]\", p_layers_8_final_layer_norm_bias: \"f32[256]\", p_layers_9_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_9_self_attn_k_proj_bias: \"f32[256]\", p_layers_9_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_9_self_attn_v_proj_bias: \"f32[256]\", p_layers_9_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_9_self_attn_q_proj_bias: \"f32[256]\", p_layers_9_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_9_self_attn_out_proj_bias: \"f32[256]\", p_layers_9_self_attn_layer_norm_weight: \"f32[256]\", p_layers_9_self_attn_layer_norm_bias: \"f32[256]\", p_layers_9_fc1_weight: \"f32[2048, 256]\", p_layers_9_fc1_bias: \"f32[2048]\", p_layers_9_fc2_weight: \"f32[256, 2048]\", p_layers_9_fc2_bias: \"f32[256]\", p_layers_9_final_layer_norm_weight: \"f32[256]\", p_layers_9_final_layer_norm_bias: \"f32[256]\", p_layers_10_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_10_self_attn_k_proj_bias: \"f32[256]\", p_layers_10_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_10_self_attn_v_proj_bias: \"f32[256]\", p_layers_10_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_10_self_attn_q_proj_bias: \"f32[256]\", p_layers_10_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_10_self_attn_out_proj_bias: \"f32[256]\", p_layers_10_self_attn_layer_norm_weight: \"f32[256]\", p_layers_10_self_attn_layer_norm_bias: \"f32[256]\", p_layers_10_fc1_weight: \"f32[2048, 256]\", p_layers_10_fc1_bias: \"f32[2048]\", p_layers_10_fc2_weight: \"f32[256, 2048]\", p_layers_10_fc2_bias: \"f32[256]\", p_layers_10_final_layer_norm_weight: \"f32[256]\", p_layers_10_final_layer_norm_bias: \"f32[256]\", p_layers_11_self_attn_k_proj_weight: \"f32[256, 256]\", p_layers_11_self_attn_k_proj_bias: \"f32[256]\", p_layers_11_self_attn_v_proj_weight: \"f32[256, 256]\", p_layers_11_self_attn_v_proj_bias: \"f32[256]\", p_layers_11_self_attn_q_proj_weight: \"f32[256, 256]\", p_layers_11_self_attn_q_proj_bias: \"f32[256]\", p_layers_11_self_attn_out_proj_weight: \"f32[256, 256]\", p_layers_11_self_attn_out_proj_bias: \"f32[256]\", p_layers_11_self_attn_layer_norm_weight: \"f32[256]\", p_layers_11_self_attn_layer_norm_bias: \"f32[256]\", p_layers_11_fc1_weight: \"f32[2048, 256]\", p_layers_11_fc1_bias: \"f32[2048]\", p_layers_11_fc2_weight: \"f32[256, 2048]\", p_layers_11_fc2_bias: \"f32[256]\", p_layers_11_final_layer_norm_weight: \"f32[256]\", p_layers_11_final_layer_norm_bias: \"f32[256]\", p_layer_norm_weight: \"f32[256]\", p_layer_norm_bias: \"f32[256]\", input_features: \"f32[s6, s37, 80]\"):\n",
       "                     # \n",
       "                    sym_size_int_103: \"Sym(s6)\" = torch.ops.aten.sym_size.int(input_features, 0)\n",
       "                    sym_size_int_104: \"Sym(s37)\" = torch.ops.aten.sym_size.int(input_features, 1)\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:97 in forward, code: hidden_states = input_features.transpose(1, 2).contiguous()  # -> B x (C x D) x T\n",
       "                    transpose: \"f32[s6, 80, s37]\" = torch.ops.aten.transpose.int(input_features, 1, 2);  input_features = None\n",
       "                    clone: \"f32[s6, 80, s37]\" = torch.ops.aten.clone.default(transpose, memory_format = torch.contiguous_format);  transpose = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv1d: \"f32[s6, 1024, (((s37 - 1)//2)) + 1]\" = torch.ops.aten.conv1d.default(clone, p_conv_conv_layers_0_weight, p_conv_conv_layers_0_bias, [2], [2]);  clone = p_conv_conv_layers_0_weight = p_conv_conv_layers_0_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:100 in forward, code: hidden_states = nn.functional.glu(hidden_states, dim=1)\n",
       "                    glu: \"f32[s6, 512, (((s37 - 1)//2)) + 1]\" = torch.ops.aten.glu.default(conv1d, 1);  conv1d = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:371 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv1d_1: \"f32[s6, 512, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.conv1d.default(glu, p_conv_conv_layers_1_weight, p_conv_conv_layers_1_bias, [2], [2]);  glu = p_conv_conv_layers_1_weight = p_conv_conv_layers_1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:100 in forward, code: hidden_states = nn.functional.glu(hidden_states, dim=1)\n",
       "                    glu_1: \"f32[s6, 256, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.glu.default(conv1d_1, 1);  conv1d_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:101 in forward, code: hidden_states = hidden_states.transpose(1, 2).contiguous()  # -> T x B x (C x D)\n",
       "                    transpose_1: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.transpose.int(glu_1, 1, 2);  glu_1 = None\n",
       "                    add_42: \"Sym(s37 - 1)\" = -1 + sym_size_int_104;  sym_size_int_104 = None\n",
       "                    floordiv_7: \"Sym(((s37 - 1)//4))\" = add_42 // 4;  add_42 = None\n",
       "                    add_43: \"Sym((((s37 - 1)//4)) + 1)\" = 1 + floordiv_7;  floordiv_7 = None\n",
       "                    clone_1: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(transpose_1, memory_format = torch.contiguous_format);  transpose_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:758 in forward, code: inputs_embeds = self.embed_scale * inputs_embeds\n",
       "                    mul_35: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(clone_1, 16.0);  clone_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:765 in forward, code: padding_mask = torch.zeros(inputs_embeds.shape[:2], dtype=torch.long, device=inputs_embeds.device)\n",
       "                    zeros: \"i64[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.zeros.default([sym_size_int_103, add_43], dtype = torch.int64, device = device(type='cpu'), pin_memory = False)\n",
       "            \n",
       "                     # File: <eval_with_key>.14:5 in forward, code: ne_3 = torch.ops.aten.ne.Scalar(zeros, 1);  zeros = None\n",
       "                    ne_7: \"b8[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.ne.Scalar(zeros, 1);  zeros = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:7 in forward, code: to = torch.ops.aten.to.dtype(ne_3, torch.int32);  ne_3 = None\n",
       "                    _to_copy: \"i32[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten._to_copy.default(ne_7, dtype = torch.int32);  ne_7 = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:8 in forward, code: cumsum = torch.ops.aten.cumsum.default(to, 1)\n",
       "                    convert_element_type_default: \"i64[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.prims.convert_element_type.default(_to_copy, dtype = torch.int64)\n",
       "                    cumsum: \"i64[1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.cumsum.default(convert_element_type_default, 1);  convert_element_type_default = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:9 in forward, code: type_as = torch.ops.aten.type_as.default(cumsum, to);  cumsum = None\n",
       "                    type_as: \"i32[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.type_as.default(cumsum, _to_copy);  cumsum = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:10 in forward, code: add = torch.ops.aten.add.Tensor(type_as, 0);  type_as = None\n",
       "                    scalar_tensor_default: \"i32[]\" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int32)\n",
       "                    add_66: \"i32[1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.add.Tensor(type_as, scalar_tensor_default);  type_as = scalar_tensor_default = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:11 in forward, code: mul_5 = torch.ops.aten.mul.Tensor(add, to);  add = to = None\n",
       "                    mul_50: \"i32[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.mul.Tensor(add_66, _to_copy);  add_66 = _to_copy = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:13 in forward, code: to_1 = torch.ops.aten.to.dtype(mul_5, torch.int64);  mul_5 = None\n",
       "                    _to_copy_1: \"i64[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten._to_copy.default(mul_50, dtype = torch.int64);  mul_50 = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:14 in forward, code: add_1 = torch.ops.aten.add.Tensor(to_1, 1);  to_1 = None\n",
       "                    add_76: \"i64[s6, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.add.Tensor(_to_copy_1, 1);  _to_copy_1 = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:17 in forward, code: view = torch.ops.aten.view.default(to_2, [-1]);  to_2 = None\n",
       "                    view: \"i64[s6*((((s37 - 1)//4)) + 1)]\" = torch.ops.aten.view.default(add_76, [-1]);  add_76 = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:18 in forward, code: index_select = torch.ops.aten.index_select.default(p_embed_positions_weights, 0, view);  p_embed_positions_weights = view = None\n",
       "                    index_select: \"f32[s6*((((s37 - 1)//4)) + 1), 256]\" = torch.ops.aten.index_select.default(p_embed_positions_weights, 0, view);  p_embed_positions_weights = view = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:19 in forward, code: view_1 = torch.ops.aten.view.default(index_select, [sym_size_int_28, add_29, -1]);  index_select = sym_size_int_28 = add_29 = None\n",
       "                    view_1: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.view.default(index_select, [sym_size_int_103, add_43, -1]);  index_select = None\n",
       "            \n",
       "                     # File: <eval_with_key>.14:20 in forward, code: detach = torch.ops.aten.detach.default(view_1);  view_1 = None\n",
       "                    detach: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.detach.default(view_1);  view_1 = None\n",
       "                    detach_1: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.detach.default(detach);  detach = None\n",
       "                    detach_2: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.detach.default(detach_1);  detach_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:769 in forward, code: hidden_states = inputs_embeds + embed_pos\n",
       "                    add_97: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(mul_35, detach_2);  mul_35 = detach_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:770 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_2: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(add_97);  add_97 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(clone_2, [256], p_layers_0_self_attn_layer_norm_weight, p_layers_0_self_attn_layer_norm_bias);  p_layers_0_self_attn_layer_norm_weight = p_layers_0_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm, p_layers_0_self_attn_q_proj_weight, p_layers_0_self_attn_q_proj_bias);  p_layers_0_self_attn_q_proj_weight = p_layers_0_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_81: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear, 0.125);  linear = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_1: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm, p_layers_0_self_attn_k_proj_weight, p_layers_0_self_attn_k_proj_bias);  p_layers_0_self_attn_k_proj_weight = p_layers_0_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_2: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_1, [sym_size_int_103, -1, 4, 64]);  linear_1 = None\n",
       "                    transpose_2: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_2, 1, 2);  view_2 = None\n",
       "                    clone_3: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_2, memory_format = torch.contiguous_format);  transpose_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm, p_layers_0_self_attn_v_proj_weight, p_layers_0_self_attn_v_proj_bias);  layer_norm = p_layers_0_self_attn_v_proj_weight = p_layers_0_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_3: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_2, [sym_size_int_103, -1, 4, 64]);  linear_2 = None\n",
       "                    transpose_3: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_3, 1, 2);  view_3 = None\n",
       "                    clone_4: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_3, memory_format = torch.contiguous_format);  transpose_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:269 in forward, code: proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
       "                    mul_119: \"Sym(4*s6)\" = sym_size_int_103 * 4\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_4: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_81, [sym_size_int_103, add_43, 4, 64]);  mul_81 = None\n",
       "                    transpose_4: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_4, 1, 2);  view_4 = None\n",
       "                    clone_5: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_4, memory_format = torch.contiguous_format);  transpose_4 = None\n",
       "                    view_5: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_5, [mul_119, -1, 64]);  clone_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_6: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_3, [mul_119, -1, 64]);  clone_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_7: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_4, [mul_119, -1, 64]);  clone_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_5: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_6, 1, 2);  view_6 = None\n",
       "                    bmm: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_5, transpose_5);  view_5 = transpose_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm, -1);  bmm = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_6: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax);  softmax = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_1: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_6, view_7);  clone_6 = view_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_8: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_1, [sym_size_int_103, 4, add_43, 64]);  bmm_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_6: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_8, 1, 2);  view_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_7: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_6, memory_format = torch.contiguous_format);  transpose_6 = None\n",
       "                    _unsafe_view: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_7, [sym_size_int_103, add_43, 256]);  clone_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view, p_layers_0_self_attn_out_proj_weight, p_layers_0_self_attn_out_proj_bias);  _unsafe_view = p_layers_0_self_attn_out_proj_weight = p_layers_0_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_8: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_213: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(clone_2, clone_8);  clone_2 = clone_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_213, [256], p_layers_0_final_layer_norm_weight, p_layers_0_final_layer_norm_bias);  p_layers_0_final_layer_norm_weight = p_layers_0_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_4: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_1, p_layers_0_fc1_weight, p_layers_0_fc1_bias);  layer_norm_1 = p_layers_0_fc1_weight = p_layers_0_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_4);  linear_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_9: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu);  relu = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_5: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_9, p_layers_0_fc2_weight, p_layers_0_fc2_bias);  clone_9 = p_layers_0_fc2_weight = p_layers_0_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_10: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_5);  linear_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_236: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_213, clone_10);  add_213 = clone_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_236, [256], p_layers_1_self_attn_layer_norm_weight, p_layers_1_self_attn_layer_norm_bias);  p_layers_1_self_attn_layer_norm_weight = p_layers_1_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_2, p_layers_1_self_attn_q_proj_weight, p_layers_1_self_attn_q_proj_bias);  p_layers_1_self_attn_q_proj_weight = p_layers_1_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_245: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_6, 0.125);  linear_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_2, p_layers_1_self_attn_k_proj_weight, p_layers_1_self_attn_k_proj_bias);  p_layers_1_self_attn_k_proj_weight = p_layers_1_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_9: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_7, [sym_size_int_103, -1, 4, 64]);  linear_7 = None\n",
       "                    transpose_7: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_9, 1, 2);  view_9 = None\n",
       "                    clone_11: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_7, memory_format = torch.contiguous_format);  transpose_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_2, p_layers_1_self_attn_v_proj_weight, p_layers_1_self_attn_v_proj_bias);  layer_norm_2 = p_layers_1_self_attn_v_proj_weight = p_layers_1_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_10: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_8, [sym_size_int_103, -1, 4, 64]);  linear_8 = None\n",
       "                    transpose_8: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_10, 1, 2);  view_10 = None\n",
       "                    clone_12: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_8, memory_format = torch.contiguous_format);  transpose_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_11: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_245, [sym_size_int_103, add_43, 4, 64]);  mul_245 = None\n",
       "                    transpose_9: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_11, 1, 2);  view_11 = None\n",
       "                    clone_13: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_9, memory_format = torch.contiguous_format);  transpose_9 = None\n",
       "                    view_12: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_13, [mul_119, -1, 64]);  clone_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_13: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_11, [mul_119, -1, 64]);  clone_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_14: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_12, [mul_119, -1, 64]);  clone_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_10: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_13, 1, 2);  view_13 = None\n",
       "                    bmm_2: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_12, transpose_10);  view_12 = transpose_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_1: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_2, -1);  bmm_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_14: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_1);  softmax_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_3: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_14, view_14);  clone_14 = view_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_15: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_3, [sym_size_int_103, 4, add_43, 64]);  bmm_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_11: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_15, 1, 2);  view_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_15: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_11, memory_format = torch.contiguous_format);  transpose_11 = None\n",
       "                    _unsafe_view_1: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_15, [sym_size_int_103, add_43, 256]);  clone_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_1, p_layers_1_self_attn_out_proj_weight, p_layers_1_self_attn_out_proj_bias);  _unsafe_view_1 = p_layers_1_self_attn_out_proj_weight = p_layers_1_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_16: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_9);  linear_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_347: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_236, clone_16);  add_236 = clone_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_347, [256], p_layers_1_final_layer_norm_weight, p_layers_1_final_layer_norm_bias);  p_layers_1_final_layer_norm_weight = p_layers_1_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_10: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_3, p_layers_1_fc1_weight, p_layers_1_fc1_bias);  layer_norm_3 = p_layers_1_fc1_weight = p_layers_1_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_10);  linear_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_17: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_1);  relu_1 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_11: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_17, p_layers_1_fc2_weight, p_layers_1_fc2_bias);  clone_17 = p_layers_1_fc2_weight = p_layers_1_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_18: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_11);  linear_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_370: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_347, clone_18);  add_347 = clone_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_4: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_370, [256], p_layers_2_self_attn_layer_norm_weight, p_layers_2_self_attn_layer_norm_bias);  p_layers_2_self_attn_layer_norm_weight = p_layers_2_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_12: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_4, p_layers_2_self_attn_q_proj_weight, p_layers_2_self_attn_q_proj_bias);  p_layers_2_self_attn_q_proj_weight = p_layers_2_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_408: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_12, 0.125);  linear_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_13: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_4, p_layers_2_self_attn_k_proj_weight, p_layers_2_self_attn_k_proj_bias);  p_layers_2_self_attn_k_proj_weight = p_layers_2_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_16: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_13, [sym_size_int_103, -1, 4, 64]);  linear_13 = None\n",
       "                    transpose_12: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_16, 1, 2);  view_16 = None\n",
       "                    clone_19: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_12, memory_format = torch.contiguous_format);  transpose_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_14: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_4, p_layers_2_self_attn_v_proj_weight, p_layers_2_self_attn_v_proj_bias);  layer_norm_4 = p_layers_2_self_attn_v_proj_weight = p_layers_2_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_17: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_14, [sym_size_int_103, -1, 4, 64]);  linear_14 = None\n",
       "                    transpose_13: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_17, 1, 2);  view_17 = None\n",
       "                    clone_20: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_13, memory_format = torch.contiguous_format);  transpose_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_18: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_408, [sym_size_int_103, add_43, 4, 64]);  mul_408 = None\n",
       "                    transpose_14: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_18, 1, 2);  view_18 = None\n",
       "                    clone_21: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_14, memory_format = torch.contiguous_format);  transpose_14 = None\n",
       "                    view_19: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_21, [mul_119, -1, 64]);  clone_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_20: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_19, [mul_119, -1, 64]);  clone_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_21: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_20, [mul_119, -1, 64]);  clone_20 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_15: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_20, 1, 2);  view_20 = None\n",
       "                    bmm_4: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_19, transpose_15);  view_19 = transpose_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_2: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_4, -1);  bmm_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_22: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_2);  softmax_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_5: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_22, view_21);  clone_22 = view_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_22: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_5, [sym_size_int_103, 4, add_43, 64]);  bmm_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_16: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_22, 1, 2);  view_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_23: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_16, memory_format = torch.contiguous_format);  transpose_16 = None\n",
       "                    _unsafe_view_2: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_23, [sym_size_int_103, add_43, 256]);  clone_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_15: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_2, p_layers_2_self_attn_out_proj_weight, p_layers_2_self_attn_out_proj_bias);  _unsafe_view_2 = p_layers_2_self_attn_out_proj_weight = p_layers_2_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_24: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_15);  linear_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_481: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_370, clone_24);  add_370 = clone_24 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_5: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_481, [256], p_layers_2_final_layer_norm_weight, p_layers_2_final_layer_norm_bias);  p_layers_2_final_layer_norm_weight = p_layers_2_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_16: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_5, p_layers_2_fc1_weight, p_layers_2_fc1_bias);  layer_norm_5 = p_layers_2_fc1_weight = p_layers_2_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_16);  linear_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_25: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_2);  relu_2 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_17: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_25, p_layers_2_fc2_weight, p_layers_2_fc2_bias);  clone_25 = p_layers_2_fc2_weight = p_layers_2_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_26: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_17);  linear_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_504: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_481, clone_26);  add_481 = clone_26 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_6: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_504, [256], p_layers_3_self_attn_layer_norm_weight, p_layers_3_self_attn_layer_norm_bias);  p_layers_3_self_attn_layer_norm_weight = p_layers_3_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_18: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_6, p_layers_3_self_attn_q_proj_weight, p_layers_3_self_attn_q_proj_bias);  p_layers_3_self_attn_q_proj_weight = p_layers_3_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_571: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_18, 0.125);  linear_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_19: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_6, p_layers_3_self_attn_k_proj_weight, p_layers_3_self_attn_k_proj_bias);  p_layers_3_self_attn_k_proj_weight = p_layers_3_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_23: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_19, [sym_size_int_103, -1, 4, 64]);  linear_19 = None\n",
       "                    transpose_17: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_23, 1, 2);  view_23 = None\n",
       "                    clone_27: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_17, memory_format = torch.contiguous_format);  transpose_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_20: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_6, p_layers_3_self_attn_v_proj_weight, p_layers_3_self_attn_v_proj_bias);  layer_norm_6 = p_layers_3_self_attn_v_proj_weight = p_layers_3_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_24: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_20, [sym_size_int_103, -1, 4, 64]);  linear_20 = None\n",
       "                    transpose_18: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_24, 1, 2);  view_24 = None\n",
       "                    clone_28: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_18, memory_format = torch.contiguous_format);  transpose_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_25: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_571, [sym_size_int_103, add_43, 4, 64]);  mul_571 = None\n",
       "                    transpose_19: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_25, 1, 2);  view_25 = None\n",
       "                    clone_29: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_19, memory_format = torch.contiguous_format);  transpose_19 = None\n",
       "                    view_26: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_29, [mul_119, -1, 64]);  clone_29 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_27: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_27, [mul_119, -1, 64]);  clone_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_28: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_28, [mul_119, -1, 64]);  clone_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_20: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_27, 1, 2);  view_27 = None\n",
       "                    bmm_6: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_26, transpose_20);  view_26 = transpose_20 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_3: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_6, -1);  bmm_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_30: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_3);  softmax_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_7: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_30, view_28);  clone_30 = view_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_29: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_7, [sym_size_int_103, 4, add_43, 64]);  bmm_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_21: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_29, 1, 2);  view_29 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_31: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_21, memory_format = torch.contiguous_format);  transpose_21 = None\n",
       "                    _unsafe_view_3: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_31, [sym_size_int_103, add_43, 256]);  clone_31 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_21: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_3, p_layers_3_self_attn_out_proj_weight, p_layers_3_self_attn_out_proj_bias);  _unsafe_view_3 = p_layers_3_self_attn_out_proj_weight = p_layers_3_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_32: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_21);  linear_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_615: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_504, clone_32);  add_504 = clone_32 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_7: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_615, [256], p_layers_3_final_layer_norm_weight, p_layers_3_final_layer_norm_bias);  p_layers_3_final_layer_norm_weight = p_layers_3_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_22: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_7, p_layers_3_fc1_weight, p_layers_3_fc1_bias);  layer_norm_7 = p_layers_3_fc1_weight = p_layers_3_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_22);  linear_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_33: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_3);  relu_3 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_23: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_33, p_layers_3_fc2_weight, p_layers_3_fc2_bias);  clone_33 = p_layers_3_fc2_weight = p_layers_3_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_34: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_23);  linear_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_638: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_615, clone_34);  add_615 = clone_34 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_8: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_638, [256], p_layers_4_self_attn_layer_norm_weight, p_layers_4_self_attn_layer_norm_bias);  p_layers_4_self_attn_layer_norm_weight = p_layers_4_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_24: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_8, p_layers_4_self_attn_q_proj_weight, p_layers_4_self_attn_q_proj_bias);  p_layers_4_self_attn_q_proj_weight = p_layers_4_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_734: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_24, 0.125);  linear_24 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_25: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_8, p_layers_4_self_attn_k_proj_weight, p_layers_4_self_attn_k_proj_bias);  p_layers_4_self_attn_k_proj_weight = p_layers_4_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_30: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_25, [sym_size_int_103, -1, 4, 64]);  linear_25 = None\n",
       "                    transpose_22: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_30, 1, 2);  view_30 = None\n",
       "                    clone_35: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_22, memory_format = torch.contiguous_format);  transpose_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_26: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_8, p_layers_4_self_attn_v_proj_weight, p_layers_4_self_attn_v_proj_bias);  layer_norm_8 = p_layers_4_self_attn_v_proj_weight = p_layers_4_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_31: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_26, [sym_size_int_103, -1, 4, 64]);  linear_26 = None\n",
       "                    transpose_23: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_31, 1, 2);  view_31 = None\n",
       "                    clone_36: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_23, memory_format = torch.contiguous_format);  transpose_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_32: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_734, [sym_size_int_103, add_43, 4, 64]);  mul_734 = None\n",
       "                    transpose_24: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_32, 1, 2);  view_32 = None\n",
       "                    clone_37: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_24, memory_format = torch.contiguous_format);  transpose_24 = None\n",
       "                    view_33: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_37, [mul_119, -1, 64]);  clone_37 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_34: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_35, [mul_119, -1, 64]);  clone_35 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_35: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_36, [mul_119, -1, 64]);  clone_36 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_25: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_34, 1, 2);  view_34 = None\n",
       "                    bmm_8: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_33, transpose_25);  view_33 = transpose_25 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_4: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_8, -1);  bmm_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_38: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_4);  softmax_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_9: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_38, view_35);  clone_38 = view_35 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_36: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_9, [sym_size_int_103, 4, add_43, 64]);  bmm_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_26: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_36, 1, 2);  view_36 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_39: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_26, memory_format = torch.contiguous_format);  transpose_26 = None\n",
       "                    _unsafe_view_4: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_39, [sym_size_int_103, add_43, 256]);  clone_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_27: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_4, p_layers_4_self_attn_out_proj_weight, p_layers_4_self_attn_out_proj_bias);  _unsafe_view_4 = p_layers_4_self_attn_out_proj_weight = p_layers_4_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_40: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_27);  linear_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_749: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_638, clone_40);  add_638 = clone_40 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_9: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_749, [256], p_layers_4_final_layer_norm_weight, p_layers_4_final_layer_norm_bias);  p_layers_4_final_layer_norm_weight = p_layers_4_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_28: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_9, p_layers_4_fc1_weight, p_layers_4_fc1_bias);  layer_norm_9 = p_layers_4_fc1_weight = p_layers_4_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_28);  linear_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_41: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_4);  relu_4 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_29: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_41, p_layers_4_fc2_weight, p_layers_4_fc2_bias);  clone_41 = p_layers_4_fc2_weight = p_layers_4_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_42: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_29);  linear_29 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_772: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_749, clone_42);  add_749 = clone_42 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_10: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_772, [256], p_layers_5_self_attn_layer_norm_weight, p_layers_5_self_attn_layer_norm_bias);  p_layers_5_self_attn_layer_norm_weight = p_layers_5_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_30: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_10, p_layers_5_self_attn_q_proj_weight, p_layers_5_self_attn_q_proj_bias);  p_layers_5_self_attn_q_proj_weight = p_layers_5_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_897: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_30, 0.125);  linear_30 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_31: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_10, p_layers_5_self_attn_k_proj_weight, p_layers_5_self_attn_k_proj_bias);  p_layers_5_self_attn_k_proj_weight = p_layers_5_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_37: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_31, [sym_size_int_103, -1, 4, 64]);  linear_31 = None\n",
       "                    transpose_27: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_37, 1, 2);  view_37 = None\n",
       "                    clone_43: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_27, memory_format = torch.contiguous_format);  transpose_27 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_32: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_10, p_layers_5_self_attn_v_proj_weight, p_layers_5_self_attn_v_proj_bias);  layer_norm_10 = p_layers_5_self_attn_v_proj_weight = p_layers_5_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_38: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_32, [sym_size_int_103, -1, 4, 64]);  linear_32 = None\n",
       "                    transpose_28: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_38, 1, 2);  view_38 = None\n",
       "                    clone_44: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_28, memory_format = torch.contiguous_format);  transpose_28 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_39: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_897, [sym_size_int_103, add_43, 4, 64]);  mul_897 = None\n",
       "                    transpose_29: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_39, 1, 2);  view_39 = None\n",
       "                    clone_45: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_29, memory_format = torch.contiguous_format);  transpose_29 = None\n",
       "                    view_40: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_45, [mul_119, -1, 64]);  clone_45 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_41: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_43, [mul_119, -1, 64]);  clone_43 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_42: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_44, [mul_119, -1, 64]);  clone_44 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_30: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_41, 1, 2);  view_41 = None\n",
       "                    bmm_10: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_40, transpose_30);  view_40 = transpose_30 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_5: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_10, -1);  bmm_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_46: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_5);  softmax_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_11: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_46, view_42);  clone_46 = view_42 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_43: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_11, [sym_size_int_103, 4, add_43, 64]);  bmm_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_31: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_43, 1, 2);  view_43 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_47: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_31, memory_format = torch.contiguous_format);  transpose_31 = None\n",
       "                    _unsafe_view_5: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_47, [sym_size_int_103, add_43, 256]);  clone_47 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_33: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_5, p_layers_5_self_attn_out_proj_weight, p_layers_5_self_attn_out_proj_bias);  _unsafe_view_5 = p_layers_5_self_attn_out_proj_weight = p_layers_5_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_48: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_33);  linear_33 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_883: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_772, clone_48);  add_772 = clone_48 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_11: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_883, [256], p_layers_5_final_layer_norm_weight, p_layers_5_final_layer_norm_bias);  p_layers_5_final_layer_norm_weight = p_layers_5_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_34: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_11, p_layers_5_fc1_weight, p_layers_5_fc1_bias);  layer_norm_11 = p_layers_5_fc1_weight = p_layers_5_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_34);  linear_34 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_49: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_5);  relu_5 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_35: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_49, p_layers_5_fc2_weight, p_layers_5_fc2_bias);  clone_49 = p_layers_5_fc2_weight = p_layers_5_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_50: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_35);  linear_35 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_906: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_883, clone_50);  add_883 = clone_50 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_12: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_906, [256], p_layers_6_self_attn_layer_norm_weight, p_layers_6_self_attn_layer_norm_bias);  p_layers_6_self_attn_layer_norm_weight = p_layers_6_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_36: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_12, p_layers_6_self_attn_q_proj_weight, p_layers_6_self_attn_q_proj_bias);  p_layers_6_self_attn_q_proj_weight = p_layers_6_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1060: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_36, 0.125);  linear_36 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_37: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_12, p_layers_6_self_attn_k_proj_weight, p_layers_6_self_attn_k_proj_bias);  p_layers_6_self_attn_k_proj_weight = p_layers_6_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_44: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_37, [sym_size_int_103, -1, 4, 64]);  linear_37 = None\n",
       "                    transpose_32: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_44, 1, 2);  view_44 = None\n",
       "                    clone_51: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_32, memory_format = torch.contiguous_format);  transpose_32 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_38: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_12, p_layers_6_self_attn_v_proj_weight, p_layers_6_self_attn_v_proj_bias);  layer_norm_12 = p_layers_6_self_attn_v_proj_weight = p_layers_6_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_45: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_38, [sym_size_int_103, -1, 4, 64]);  linear_38 = None\n",
       "                    transpose_33: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_45, 1, 2);  view_45 = None\n",
       "                    clone_52: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_33, memory_format = torch.contiguous_format);  transpose_33 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_46: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1060, [sym_size_int_103, add_43, 4, 64]);  mul_1060 = None\n",
       "                    transpose_34: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_46, 1, 2);  view_46 = None\n",
       "                    clone_53: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_34, memory_format = torch.contiguous_format);  transpose_34 = None\n",
       "                    view_47: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_53, [mul_119, -1, 64]);  clone_53 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_48: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_51, [mul_119, -1, 64]);  clone_51 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_49: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_52, [mul_119, -1, 64]);  clone_52 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_35: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_48, 1, 2);  view_48 = None\n",
       "                    bmm_12: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_47, transpose_35);  view_47 = transpose_35 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_6: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_12, -1);  bmm_12 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_54: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_6);  softmax_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_13: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_54, view_49);  clone_54 = view_49 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_50: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_13, [sym_size_int_103, 4, add_43, 64]);  bmm_13 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_36: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_50, 1, 2);  view_50 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_55: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_36, memory_format = torch.contiguous_format);  transpose_36 = None\n",
       "                    _unsafe_view_6: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_55, [sym_size_int_103, add_43, 256]);  clone_55 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_39: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_6, p_layers_6_self_attn_out_proj_weight, p_layers_6_self_attn_out_proj_bias);  _unsafe_view_6 = p_layers_6_self_attn_out_proj_weight = p_layers_6_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_56: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_39);  linear_39 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1017: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_906, clone_56);  add_906 = clone_56 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_13: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1017, [256], p_layers_6_final_layer_norm_weight, p_layers_6_final_layer_norm_bias);  p_layers_6_final_layer_norm_weight = p_layers_6_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_40: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_13, p_layers_6_fc1_weight, p_layers_6_fc1_bias);  layer_norm_13 = p_layers_6_fc1_weight = p_layers_6_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_40);  linear_40 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_57: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_6);  relu_6 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_41: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_57, p_layers_6_fc2_weight, p_layers_6_fc2_bias);  clone_57 = p_layers_6_fc2_weight = p_layers_6_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_58: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_41);  linear_41 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1040: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1017, clone_58);  add_1017 = clone_58 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_14: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1040, [256], p_layers_7_self_attn_layer_norm_weight, p_layers_7_self_attn_layer_norm_bias);  p_layers_7_self_attn_layer_norm_weight = p_layers_7_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_42: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_14, p_layers_7_self_attn_q_proj_weight, p_layers_7_self_attn_q_proj_bias);  p_layers_7_self_attn_q_proj_weight = p_layers_7_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1223: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_42, 0.125);  linear_42 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_43: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_14, p_layers_7_self_attn_k_proj_weight, p_layers_7_self_attn_k_proj_bias);  p_layers_7_self_attn_k_proj_weight = p_layers_7_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_51: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_43, [sym_size_int_103, -1, 4, 64]);  linear_43 = None\n",
       "                    transpose_37: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_51, 1, 2);  view_51 = None\n",
       "                    clone_59: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_37, memory_format = torch.contiguous_format);  transpose_37 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_44: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_14, p_layers_7_self_attn_v_proj_weight, p_layers_7_self_attn_v_proj_bias);  layer_norm_14 = p_layers_7_self_attn_v_proj_weight = p_layers_7_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_52: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_44, [sym_size_int_103, -1, 4, 64]);  linear_44 = None\n",
       "                    transpose_38: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_52, 1, 2);  view_52 = None\n",
       "                    clone_60: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_38, memory_format = torch.contiguous_format);  transpose_38 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_53: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1223, [sym_size_int_103, add_43, 4, 64]);  mul_1223 = None\n",
       "                    transpose_39: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_53, 1, 2);  view_53 = None\n",
       "                    clone_61: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_39, memory_format = torch.contiguous_format);  transpose_39 = None\n",
       "                    view_54: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_61, [mul_119, -1, 64]);  clone_61 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_55: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_59, [mul_119, -1, 64]);  clone_59 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_56: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_60, [mul_119, -1, 64]);  clone_60 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_40: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_55, 1, 2);  view_55 = None\n",
       "                    bmm_14: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_54, transpose_40);  view_54 = transpose_40 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_7: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_14, -1);  bmm_14 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_62: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_7);  softmax_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_15: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_62, view_56);  clone_62 = view_56 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_57: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_15, [sym_size_int_103, 4, add_43, 64]);  bmm_15 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_41: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_57, 1, 2);  view_57 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_63: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_41, memory_format = torch.contiguous_format);  transpose_41 = None\n",
       "                    _unsafe_view_7: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_63, [sym_size_int_103, add_43, 256]);  clone_63 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_45: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_7, p_layers_7_self_attn_out_proj_weight, p_layers_7_self_attn_out_proj_bias);  _unsafe_view_7 = p_layers_7_self_attn_out_proj_weight = p_layers_7_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_64: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_45);  linear_45 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1151: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1040, clone_64);  add_1040 = clone_64 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_15: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1151, [256], p_layers_7_final_layer_norm_weight, p_layers_7_final_layer_norm_bias);  p_layers_7_final_layer_norm_weight = p_layers_7_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_46: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_15, p_layers_7_fc1_weight, p_layers_7_fc1_bias);  layer_norm_15 = p_layers_7_fc1_weight = p_layers_7_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_46);  linear_46 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_65: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_7);  relu_7 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_47: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_65, p_layers_7_fc2_weight, p_layers_7_fc2_bias);  clone_65 = p_layers_7_fc2_weight = p_layers_7_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_66: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_47);  linear_47 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1174: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1151, clone_66);  add_1151 = clone_66 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_16: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1174, [256], p_layers_8_self_attn_layer_norm_weight, p_layers_8_self_attn_layer_norm_bias);  p_layers_8_self_attn_layer_norm_weight = p_layers_8_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_48: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_16, p_layers_8_self_attn_q_proj_weight, p_layers_8_self_attn_q_proj_bias);  p_layers_8_self_attn_q_proj_weight = p_layers_8_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1386: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_48, 0.125);  linear_48 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_49: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_16, p_layers_8_self_attn_k_proj_weight, p_layers_8_self_attn_k_proj_bias);  p_layers_8_self_attn_k_proj_weight = p_layers_8_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_58: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_49, [sym_size_int_103, -1, 4, 64]);  linear_49 = None\n",
       "                    transpose_42: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_58, 1, 2);  view_58 = None\n",
       "                    clone_67: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_42, memory_format = torch.contiguous_format);  transpose_42 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_50: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_16, p_layers_8_self_attn_v_proj_weight, p_layers_8_self_attn_v_proj_bias);  layer_norm_16 = p_layers_8_self_attn_v_proj_weight = p_layers_8_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_59: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_50, [sym_size_int_103, -1, 4, 64]);  linear_50 = None\n",
       "                    transpose_43: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_59, 1, 2);  view_59 = None\n",
       "                    clone_68: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_43, memory_format = torch.contiguous_format);  transpose_43 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_60: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1386, [sym_size_int_103, add_43, 4, 64]);  mul_1386 = None\n",
       "                    transpose_44: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_60, 1, 2);  view_60 = None\n",
       "                    clone_69: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_44, memory_format = torch.contiguous_format);  transpose_44 = None\n",
       "                    view_61: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_69, [mul_119, -1, 64]);  clone_69 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_62: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_67, [mul_119, -1, 64]);  clone_67 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_63: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_68, [mul_119, -1, 64]);  clone_68 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_45: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_62, 1, 2);  view_62 = None\n",
       "                    bmm_16: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_61, transpose_45);  view_61 = transpose_45 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_8: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_16, -1);  bmm_16 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_70: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_8);  softmax_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_17: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_70, view_63);  clone_70 = view_63 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_64: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_17, [sym_size_int_103, 4, add_43, 64]);  bmm_17 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_46: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_64, 1, 2);  view_64 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_71: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_46, memory_format = torch.contiguous_format);  transpose_46 = None\n",
       "                    _unsafe_view_8: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_71, [sym_size_int_103, add_43, 256]);  clone_71 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_51: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_8, p_layers_8_self_attn_out_proj_weight, p_layers_8_self_attn_out_proj_bias);  _unsafe_view_8 = p_layers_8_self_attn_out_proj_weight = p_layers_8_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_72: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_51);  linear_51 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1285: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1174, clone_72);  add_1174 = clone_72 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_17: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1285, [256], p_layers_8_final_layer_norm_weight, p_layers_8_final_layer_norm_bias);  p_layers_8_final_layer_norm_weight = p_layers_8_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_52: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_17, p_layers_8_fc1_weight, p_layers_8_fc1_bias);  layer_norm_17 = p_layers_8_fc1_weight = p_layers_8_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_52);  linear_52 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_73: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_8);  relu_8 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_53: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_73, p_layers_8_fc2_weight, p_layers_8_fc2_bias);  clone_73 = p_layers_8_fc2_weight = p_layers_8_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_74: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_53);  linear_53 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1308: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1285, clone_74);  add_1285 = clone_74 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_18: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1308, [256], p_layers_9_self_attn_layer_norm_weight, p_layers_9_self_attn_layer_norm_bias);  p_layers_9_self_attn_layer_norm_weight = p_layers_9_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_54: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_18, p_layers_9_self_attn_q_proj_weight, p_layers_9_self_attn_q_proj_bias);  p_layers_9_self_attn_q_proj_weight = p_layers_9_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1549: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_54, 0.125);  linear_54 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_55: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_18, p_layers_9_self_attn_k_proj_weight, p_layers_9_self_attn_k_proj_bias);  p_layers_9_self_attn_k_proj_weight = p_layers_9_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_65: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_55, [sym_size_int_103, -1, 4, 64]);  linear_55 = None\n",
       "                    transpose_47: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_65, 1, 2);  view_65 = None\n",
       "                    clone_75: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_47, memory_format = torch.contiguous_format);  transpose_47 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_56: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_18, p_layers_9_self_attn_v_proj_weight, p_layers_9_self_attn_v_proj_bias);  layer_norm_18 = p_layers_9_self_attn_v_proj_weight = p_layers_9_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_66: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_56, [sym_size_int_103, -1, 4, 64]);  linear_56 = None\n",
       "                    transpose_48: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_66, 1, 2);  view_66 = None\n",
       "                    clone_76: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_48, memory_format = torch.contiguous_format);  transpose_48 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_67: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1549, [sym_size_int_103, add_43, 4, 64]);  mul_1549 = None\n",
       "                    transpose_49: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_67, 1, 2);  view_67 = None\n",
       "                    clone_77: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_49, memory_format = torch.contiguous_format);  transpose_49 = None\n",
       "                    view_68: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_77, [mul_119, -1, 64]);  clone_77 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_69: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_75, [mul_119, -1, 64]);  clone_75 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_70: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_76, [mul_119, -1, 64]);  clone_76 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_50: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_69, 1, 2);  view_69 = None\n",
       "                    bmm_18: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_68, transpose_50);  view_68 = transpose_50 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_9: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_18, -1);  bmm_18 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_78: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_9);  softmax_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_19: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_78, view_70);  clone_78 = view_70 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_71: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_19, [sym_size_int_103, 4, add_43, 64]);  bmm_19 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_51: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_71, 1, 2);  view_71 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_79: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_51, memory_format = torch.contiguous_format);  transpose_51 = None\n",
       "                    _unsafe_view_9: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_79, [sym_size_int_103, add_43, 256]);  clone_79 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_57: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_9, p_layers_9_self_attn_out_proj_weight, p_layers_9_self_attn_out_proj_bias);  _unsafe_view_9 = p_layers_9_self_attn_out_proj_weight = p_layers_9_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_80: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_57);  linear_57 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1419: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1308, clone_80);  add_1308 = clone_80 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_19: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1419, [256], p_layers_9_final_layer_norm_weight, p_layers_9_final_layer_norm_bias);  p_layers_9_final_layer_norm_weight = p_layers_9_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_58: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_19, p_layers_9_fc1_weight, p_layers_9_fc1_bias);  layer_norm_19 = p_layers_9_fc1_weight = p_layers_9_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_58);  linear_58 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_81: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_9);  relu_9 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_59: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_81, p_layers_9_fc2_weight, p_layers_9_fc2_bias);  clone_81 = p_layers_9_fc2_weight = p_layers_9_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_82: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_59);  linear_59 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1442: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1419, clone_82);  add_1419 = clone_82 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_20: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1442, [256], p_layers_10_self_attn_layer_norm_weight, p_layers_10_self_attn_layer_norm_bias);  p_layers_10_self_attn_layer_norm_weight = p_layers_10_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_60: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_20, p_layers_10_self_attn_q_proj_weight, p_layers_10_self_attn_q_proj_bias);  p_layers_10_self_attn_q_proj_weight = p_layers_10_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1712: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_60, 0.125);  linear_60 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_61: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_20, p_layers_10_self_attn_k_proj_weight, p_layers_10_self_attn_k_proj_bias);  p_layers_10_self_attn_k_proj_weight = p_layers_10_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_72: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_61, [sym_size_int_103, -1, 4, 64]);  linear_61 = None\n",
       "                    transpose_52: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_72, 1, 2);  view_72 = None\n",
       "                    clone_83: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_52, memory_format = torch.contiguous_format);  transpose_52 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_62: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_20, p_layers_10_self_attn_v_proj_weight, p_layers_10_self_attn_v_proj_bias);  layer_norm_20 = p_layers_10_self_attn_v_proj_weight = p_layers_10_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_73: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_62, [sym_size_int_103, -1, 4, 64]);  linear_62 = None\n",
       "                    transpose_53: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_73, 1, 2);  view_73 = None\n",
       "                    clone_84: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_53, memory_format = torch.contiguous_format);  transpose_53 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_74: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1712, [sym_size_int_103, add_43, 4, 64]);  mul_1712 = None\n",
       "                    transpose_54: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_74, 1, 2);  view_74 = None\n",
       "                    clone_85: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_54, memory_format = torch.contiguous_format);  transpose_54 = None\n",
       "                    view_75: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_85, [mul_119, -1, 64]);  clone_85 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_76: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_83, [mul_119, -1, 64]);  clone_83 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_77: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_84, [mul_119, -1, 64]);  clone_84 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_55: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_76, 1, 2);  view_76 = None\n",
       "                    bmm_20: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_75, transpose_55);  view_75 = transpose_55 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_10: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_20, -1);  bmm_20 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_86: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_10);  softmax_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_21: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_86, view_77);  clone_86 = view_77 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_78: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_21, [sym_size_int_103, 4, add_43, 64]);  bmm_21 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_56: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_78, 1, 2);  view_78 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_87: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_56, memory_format = torch.contiguous_format);  transpose_56 = None\n",
       "                    _unsafe_view_10: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_87, [sym_size_int_103, add_43, 256]);  clone_87 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_63: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_10, p_layers_10_self_attn_out_proj_weight, p_layers_10_self_attn_out_proj_bias);  _unsafe_view_10 = p_layers_10_self_attn_out_proj_weight = p_layers_10_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_88: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_63);  linear_63 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1553: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1442, clone_88);  add_1442 = clone_88 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_21: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1553, [256], p_layers_10_final_layer_norm_weight, p_layers_10_final_layer_norm_bias);  p_layers_10_final_layer_norm_weight = p_layers_10_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_64: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_21, p_layers_10_fc1_weight, p_layers_10_fc1_bias);  layer_norm_21 = p_layers_10_fc1_weight = p_layers_10_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_10: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_64);  linear_64 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_89: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_10);  relu_10 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_65: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_89, p_layers_10_fc2_weight, p_layers_10_fc2_bias);  clone_89 = p_layers_10_fc2_weight = p_layers_10_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_90: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_65);  linear_65 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1576: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1553, clone_90);  add_1553 = clone_90 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_22: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1576, [256], p_layers_11_self_attn_layer_norm_weight, p_layers_11_self_attn_layer_norm_bias);  p_layers_11_self_attn_layer_norm_weight = p_layers_11_self_attn_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_66: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_22, p_layers_11_self_attn_q_proj_weight, p_layers_11_self_attn_q_proj_bias);  p_layers_11_self_attn_q_proj_weight = p_layers_11_self_attn_q_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:231 in forward, code: query_states = self.q_proj(hidden_states) * self.scaling\n",
       "                    mul_1875: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.mul.Tensor(linear_66, 0.125);  linear_66 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_67: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_22, p_layers_11_self_attn_k_proj_weight, p_layers_11_self_attn_k_proj_bias);  p_layers_11_self_attn_k_proj_weight = p_layers_11_self_attn_k_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:256 in forward, code: key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
       "                    view_79: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_67, [sym_size_int_103, -1, 4, 64]);  linear_67 = None\n",
       "                    transpose_57: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_79, 1, 2);  view_79 = None\n",
       "                    clone_91: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_57, memory_format = torch.contiguous_format);  transpose_57 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_68: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(layer_norm_22, p_layers_11_self_attn_v_proj_weight, p_layers_11_self_attn_v_proj_bias);  layer_norm_22 = p_layers_11_self_attn_v_proj_weight = p_layers_11_self_attn_v_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:257 in forward, code: value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
       "                    view_80: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(linear_68, [sym_size_int_103, -1, 4, 64]);  linear_68 = None\n",
       "                    transpose_58: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_80, 1, 2);  view_80 = None\n",
       "                    clone_92: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_58, memory_format = torch.contiguous_format);  transpose_58 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:270 in forward, code: query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
       "                    view_81: \"f32[1, (((s37 - 1)//4)) + 1, 4, 64]\" = torch.ops.aten.view.default(mul_1875, [sym_size_int_103, add_43, 4, 64]);  mul_1875 = None\n",
       "                    transpose_59: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.transpose.int(view_81, 1, 2);  view_81 = None\n",
       "                    clone_93: \"f32[1, 4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.clone.default(transpose_59, memory_format = torch.contiguous_format);  transpose_59 = None\n",
       "                    view_82: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_93, [mul_119, -1, 64]);  clone_93 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:271 in forward, code: key_states = key_states.reshape(*proj_shape)\n",
       "                    view_83: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_91, [mul_119, -1, 64]);  clone_91 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:272 in forward, code: value_states = value_states.reshape(*proj_shape)\n",
       "                    view_84: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(clone_92, [mul_119, -1, 64]);  clone_92 = mul_119 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:275 in forward, code: attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
       "                    transpose_60: \"f32[4, 64, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.transpose.int(view_83, 1, 2);  view_83 = None\n",
       "                    bmm_22: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.bmm.default(view_82, transpose_60);  view_82 = transpose_60 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:291 in forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
       "                    softmax_11: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.softmax.int(bmm_22, -1);  bmm_22 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:312 in forward, code: attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
       "                    clone_94: \"f32[4, (((s37 - 1)//4)) + 1, (((s37 - 1)//4)) + 1]\" = torch.ops.aten.clone.default(softmax_11);  softmax_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:314 in forward, code: attn_output = torch.bmm(attn_probs, value_states)\n",
       "                    bmm_23: \"f32[4, (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.bmm.default(clone_94, view_84);  clone_94 = view_84 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:322 in forward, code: attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
       "                    view_85: \"f32[s6, (4//s6), (((s37 - 1)//4)) + 1, 64]\" = torch.ops.aten.view.default(bmm_23, [sym_size_int_103, 4, add_43, 64]);  bmm_23 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:323 in forward, code: attn_output = attn_output.transpose(1, 2)\n",
       "                    transpose_61: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.transpose.int(view_85, 1, 2);  view_85 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:327 in forward, code: attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n",
       "                    clone_95: \"f32[s6, (((s37 - 1)//4)) + 1, (4//s6), 64]\" = torch.ops.aten.clone.default(transpose_61, memory_format = torch.contiguous_format);  transpose_61 = None\n",
       "                    _unsafe_view_11: \"f32[s6, (((s37 - 1)//4)) + 1, 64*((4//s6))]\" = torch.ops.aten._unsafe_view.default(clone_95, [sym_size_int_103, add_43, 256]);  clone_95 = sym_size_int_103 = add_43 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_69: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.linear.default(_unsafe_view_11, p_layers_11_self_attn_out_proj_weight, p_layers_11_self_attn_out_proj_bias);  _unsafe_view_11 = p_layers_11_self_attn_out_proj_weight = p_layers_11_self_attn_out_proj_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:383 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_96: \"f32[s6, (((((s37 - 1)//4)) + 1)//s6), 256]\" = torch.ops.aten.clone.default(linear_69);  linear_69 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:384 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1687: \"f32[s6, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1576, clone_96);  add_1576 = clone_96 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_23: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1687, [256], p_layers_11_final_layer_norm_weight, p_layers_11_final_layer_norm_bias);  p_layers_11_final_layer_norm_weight = p_layers_11_final_layer_norm_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_70: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.linear.default(layer_norm_23, p_layers_11_fc1_weight, p_layers_11_fc1_bias);  layer_norm_23 = p_layers_11_fc1_weight = p_layers_11_fc1_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_11: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.relu.default(linear_70);  linear_70 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:389 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
       "                    clone_97: \"f32[1, (((s37 - 1)//4)) + 1, 2048]\" = torch.ops.aten.clone.default(relu_11);  relu_11 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_71: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.linear.default(clone_97, p_layers_11_fc2_weight, p_layers_11_fc2_bias);  clone_97 = p_layers_11_fc2_weight = p_layers_11_fc2_bias = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:391 in forward, code: hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
       "                    clone_98: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.clone.default(linear_71);  linear_71 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:392 in forward, code: hidden_states = residual + hidden_states\n",
       "                    add_1710: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.add.Tensor(add_1687, clone_98);  add_1687 = clone_98 = None\n",
       "            \n",
       "                     # File: c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:229 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_24: \"f32[1, (((s37 - 1)//4)) + 1, 256]\" = torch.ops.aten.layer_norm.default(add_1710, [256], p_layer_norm_weight, p_layer_norm_bias);  add_1710 = p_layer_norm_weight = p_layer_norm_bias = None\n",
       "                    return (layer_norm_24,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_conv_conv_layers_0_weight: PARAMETER target='conv.conv_layers.0.weight'\n",
       "            p_conv_conv_layers_0_bias: PARAMETER target='conv.conv_layers.0.bias'\n",
       "            p_conv_conv_layers_1_weight: PARAMETER target='conv.conv_layers.1.weight'\n",
       "            p_conv_conv_layers_1_bias: PARAMETER target='conv.conv_layers.1.bias'\n",
       "            p_embed_positions_weights: PARAMETER target='embed_positions.weights'\n",
       "            p_layers_0_self_attn_k_proj_weight: PARAMETER target='layers.0.self_attn.k_proj.weight'\n",
       "            p_layers_0_self_attn_k_proj_bias: PARAMETER target='layers.0.self_attn.k_proj.bias'\n",
       "            p_layers_0_self_attn_v_proj_weight: PARAMETER target='layers.0.self_attn.v_proj.weight'\n",
       "            p_layers_0_self_attn_v_proj_bias: PARAMETER target='layers.0.self_attn.v_proj.bias'\n",
       "            p_layers_0_self_attn_q_proj_weight: PARAMETER target='layers.0.self_attn.q_proj.weight'\n",
       "            p_layers_0_self_attn_q_proj_bias: PARAMETER target='layers.0.self_attn.q_proj.bias'\n",
       "            p_layers_0_self_attn_out_proj_weight: PARAMETER target='layers.0.self_attn.out_proj.weight'\n",
       "            p_layers_0_self_attn_out_proj_bias: PARAMETER target='layers.0.self_attn.out_proj.bias'\n",
       "            p_layers_0_self_attn_layer_norm_weight: PARAMETER target='layers.0.self_attn_layer_norm.weight'\n",
       "            p_layers_0_self_attn_layer_norm_bias: PARAMETER target='layers.0.self_attn_layer_norm.bias'\n",
       "            p_layers_0_fc1_weight: PARAMETER target='layers.0.fc1.weight'\n",
       "            p_layers_0_fc1_bias: PARAMETER target='layers.0.fc1.bias'\n",
       "            p_layers_0_fc2_weight: PARAMETER target='layers.0.fc2.weight'\n",
       "            p_layers_0_fc2_bias: PARAMETER target='layers.0.fc2.bias'\n",
       "            p_layers_0_final_layer_norm_weight: PARAMETER target='layers.0.final_layer_norm.weight'\n",
       "            p_layers_0_final_layer_norm_bias: PARAMETER target='layers.0.final_layer_norm.bias'\n",
       "            p_layers_1_self_attn_k_proj_weight: PARAMETER target='layers.1.self_attn.k_proj.weight'\n",
       "            p_layers_1_self_attn_k_proj_bias: PARAMETER target='layers.1.self_attn.k_proj.bias'\n",
       "            p_layers_1_self_attn_v_proj_weight: PARAMETER target='layers.1.self_attn.v_proj.weight'\n",
       "            p_layers_1_self_attn_v_proj_bias: PARAMETER target='layers.1.self_attn.v_proj.bias'\n",
       "            p_layers_1_self_attn_q_proj_weight: PARAMETER target='layers.1.self_attn.q_proj.weight'\n",
       "            p_layers_1_self_attn_q_proj_bias: PARAMETER target='layers.1.self_attn.q_proj.bias'\n",
       "            p_layers_1_self_attn_out_proj_weight: PARAMETER target='layers.1.self_attn.out_proj.weight'\n",
       "            p_layers_1_self_attn_out_proj_bias: PARAMETER target='layers.1.self_attn.out_proj.bias'\n",
       "            p_layers_1_self_attn_layer_norm_weight: PARAMETER target='layers.1.self_attn_layer_norm.weight'\n",
       "            p_layers_1_self_attn_layer_norm_bias: PARAMETER target='layers.1.self_attn_layer_norm.bias'\n",
       "            p_layers_1_fc1_weight: PARAMETER target='layers.1.fc1.weight'\n",
       "            p_layers_1_fc1_bias: PARAMETER target='layers.1.fc1.bias'\n",
       "            p_layers_1_fc2_weight: PARAMETER target='layers.1.fc2.weight'\n",
       "            p_layers_1_fc2_bias: PARAMETER target='layers.1.fc2.bias'\n",
       "            p_layers_1_final_layer_norm_weight: PARAMETER target='layers.1.final_layer_norm.weight'\n",
       "            p_layers_1_final_layer_norm_bias: PARAMETER target='layers.1.final_layer_norm.bias'\n",
       "            p_layers_2_self_attn_k_proj_weight: PARAMETER target='layers.2.self_attn.k_proj.weight'\n",
       "            p_layers_2_self_attn_k_proj_bias: PARAMETER target='layers.2.self_attn.k_proj.bias'\n",
       "            p_layers_2_self_attn_v_proj_weight: PARAMETER target='layers.2.self_attn.v_proj.weight'\n",
       "            p_layers_2_self_attn_v_proj_bias: PARAMETER target='layers.2.self_attn.v_proj.bias'\n",
       "            p_layers_2_self_attn_q_proj_weight: PARAMETER target='layers.2.self_attn.q_proj.weight'\n",
       "            p_layers_2_self_attn_q_proj_bias: PARAMETER target='layers.2.self_attn.q_proj.bias'\n",
       "            p_layers_2_self_attn_out_proj_weight: PARAMETER target='layers.2.self_attn.out_proj.weight'\n",
       "            p_layers_2_self_attn_out_proj_bias: PARAMETER target='layers.2.self_attn.out_proj.bias'\n",
       "            p_layers_2_self_attn_layer_norm_weight: PARAMETER target='layers.2.self_attn_layer_norm.weight'\n",
       "            p_layers_2_self_attn_layer_norm_bias: PARAMETER target='layers.2.self_attn_layer_norm.bias'\n",
       "            p_layers_2_fc1_weight: PARAMETER target='layers.2.fc1.weight'\n",
       "            p_layers_2_fc1_bias: PARAMETER target='layers.2.fc1.bias'\n",
       "            p_layers_2_fc2_weight: PARAMETER target='layers.2.fc2.weight'\n",
       "            p_layers_2_fc2_bias: PARAMETER target='layers.2.fc2.bias'\n",
       "            p_layers_2_final_layer_norm_weight: PARAMETER target='layers.2.final_layer_norm.weight'\n",
       "            p_layers_2_final_layer_norm_bias: PARAMETER target='layers.2.final_layer_norm.bias'\n",
       "            p_layers_3_self_attn_k_proj_weight: PARAMETER target='layers.3.self_attn.k_proj.weight'\n",
       "            p_layers_3_self_attn_k_proj_bias: PARAMETER target='layers.3.self_attn.k_proj.bias'\n",
       "            p_layers_3_self_attn_v_proj_weight: PARAMETER target='layers.3.self_attn.v_proj.weight'\n",
       "            p_layers_3_self_attn_v_proj_bias: PARAMETER target='layers.3.self_attn.v_proj.bias'\n",
       "            p_layers_3_self_attn_q_proj_weight: PARAMETER target='layers.3.self_attn.q_proj.weight'\n",
       "            p_layers_3_self_attn_q_proj_bias: PARAMETER target='layers.3.self_attn.q_proj.bias'\n",
       "            p_layers_3_self_attn_out_proj_weight: PARAMETER target='layers.3.self_attn.out_proj.weight'\n",
       "            p_layers_3_self_attn_out_proj_bias: PARAMETER target='layers.3.self_attn.out_proj.bias'\n",
       "            p_layers_3_self_attn_layer_norm_weight: PARAMETER target='layers.3.self_attn_layer_norm.weight'\n",
       "            p_layers_3_self_attn_layer_norm_bias: PARAMETER target='layers.3.self_attn_layer_norm.bias'\n",
       "            p_layers_3_fc1_weight: PARAMETER target='layers.3.fc1.weight'\n",
       "            p_layers_3_fc1_bias: PARAMETER target='layers.3.fc1.bias'\n",
       "            p_layers_3_fc2_weight: PARAMETER target='layers.3.fc2.weight'\n",
       "            p_layers_3_fc2_bias: PARAMETER target='layers.3.fc2.bias'\n",
       "            p_layers_3_final_layer_norm_weight: PARAMETER target='layers.3.final_layer_norm.weight'\n",
       "            p_layers_3_final_layer_norm_bias: PARAMETER target='layers.3.final_layer_norm.bias'\n",
       "            p_layers_4_self_attn_k_proj_weight: PARAMETER target='layers.4.self_attn.k_proj.weight'\n",
       "            p_layers_4_self_attn_k_proj_bias: PARAMETER target='layers.4.self_attn.k_proj.bias'\n",
       "            p_layers_4_self_attn_v_proj_weight: PARAMETER target='layers.4.self_attn.v_proj.weight'\n",
       "            p_layers_4_self_attn_v_proj_bias: PARAMETER target='layers.4.self_attn.v_proj.bias'\n",
       "            p_layers_4_self_attn_q_proj_weight: PARAMETER target='layers.4.self_attn.q_proj.weight'\n",
       "            p_layers_4_self_attn_q_proj_bias: PARAMETER target='layers.4.self_attn.q_proj.bias'\n",
       "            p_layers_4_self_attn_out_proj_weight: PARAMETER target='layers.4.self_attn.out_proj.weight'\n",
       "            p_layers_4_self_attn_out_proj_bias: PARAMETER target='layers.4.self_attn.out_proj.bias'\n",
       "            p_layers_4_self_attn_layer_norm_weight: PARAMETER target='layers.4.self_attn_layer_norm.weight'\n",
       "            p_layers_4_self_attn_layer_norm_bias: PARAMETER target='layers.4.self_attn_layer_norm.bias'\n",
       "            p_layers_4_fc1_weight: PARAMETER target='layers.4.fc1.weight'\n",
       "            p_layers_4_fc1_bias: PARAMETER target='layers.4.fc1.bias'\n",
       "            p_layers_4_fc2_weight: PARAMETER target='layers.4.fc2.weight'\n",
       "            p_layers_4_fc2_bias: PARAMETER target='layers.4.fc2.bias'\n",
       "            p_layers_4_final_layer_norm_weight: PARAMETER target='layers.4.final_layer_norm.weight'\n",
       "            p_layers_4_final_layer_norm_bias: PARAMETER target='layers.4.final_layer_norm.bias'\n",
       "            p_layers_5_self_attn_k_proj_weight: PARAMETER target='layers.5.self_attn.k_proj.weight'\n",
       "            p_layers_5_self_attn_k_proj_bias: PARAMETER target='layers.5.self_attn.k_proj.bias'\n",
       "            p_layers_5_self_attn_v_proj_weight: PARAMETER target='layers.5.self_attn.v_proj.weight'\n",
       "            p_layers_5_self_attn_v_proj_bias: PARAMETER target='layers.5.self_attn.v_proj.bias'\n",
       "            p_layers_5_self_attn_q_proj_weight: PARAMETER target='layers.5.self_attn.q_proj.weight'\n",
       "            p_layers_5_self_attn_q_proj_bias: PARAMETER target='layers.5.self_attn.q_proj.bias'\n",
       "            p_layers_5_self_attn_out_proj_weight: PARAMETER target='layers.5.self_attn.out_proj.weight'\n",
       "            p_layers_5_self_attn_out_proj_bias: PARAMETER target='layers.5.self_attn.out_proj.bias'\n",
       "            p_layers_5_self_attn_layer_norm_weight: PARAMETER target='layers.5.self_attn_layer_norm.weight'\n",
       "            p_layers_5_self_attn_layer_norm_bias: PARAMETER target='layers.5.self_attn_layer_norm.bias'\n",
       "            p_layers_5_fc1_weight: PARAMETER target='layers.5.fc1.weight'\n",
       "            p_layers_5_fc1_bias: PARAMETER target='layers.5.fc1.bias'\n",
       "            p_layers_5_fc2_weight: PARAMETER target='layers.5.fc2.weight'\n",
       "            p_layers_5_fc2_bias: PARAMETER target='layers.5.fc2.bias'\n",
       "            p_layers_5_final_layer_norm_weight: PARAMETER target='layers.5.final_layer_norm.weight'\n",
       "            p_layers_5_final_layer_norm_bias: PARAMETER target='layers.5.final_layer_norm.bias'\n",
       "            p_layers_6_self_attn_k_proj_weight: PARAMETER target='layers.6.self_attn.k_proj.weight'\n",
       "            p_layers_6_self_attn_k_proj_bias: PARAMETER target='layers.6.self_attn.k_proj.bias'\n",
       "            p_layers_6_self_attn_v_proj_weight: PARAMETER target='layers.6.self_attn.v_proj.weight'\n",
       "            p_layers_6_self_attn_v_proj_bias: PARAMETER target='layers.6.self_attn.v_proj.bias'\n",
       "            p_layers_6_self_attn_q_proj_weight: PARAMETER target='layers.6.self_attn.q_proj.weight'\n",
       "            p_layers_6_self_attn_q_proj_bias: PARAMETER target='layers.6.self_attn.q_proj.bias'\n",
       "            p_layers_6_self_attn_out_proj_weight: PARAMETER target='layers.6.self_attn.out_proj.weight'\n",
       "            p_layers_6_self_attn_out_proj_bias: PARAMETER target='layers.6.self_attn.out_proj.bias'\n",
       "            p_layers_6_self_attn_layer_norm_weight: PARAMETER target='layers.6.self_attn_layer_norm.weight'\n",
       "            p_layers_6_self_attn_layer_norm_bias: PARAMETER target='layers.6.self_attn_layer_norm.bias'\n",
       "            p_layers_6_fc1_weight: PARAMETER target='layers.6.fc1.weight'\n",
       "            p_layers_6_fc1_bias: PARAMETER target='layers.6.fc1.bias'\n",
       "            p_layers_6_fc2_weight: PARAMETER target='layers.6.fc2.weight'\n",
       "            p_layers_6_fc2_bias: PARAMETER target='layers.6.fc2.bias'\n",
       "            p_layers_6_final_layer_norm_weight: PARAMETER target='layers.6.final_layer_norm.weight'\n",
       "            p_layers_6_final_layer_norm_bias: PARAMETER target='layers.6.final_layer_norm.bias'\n",
       "            p_layers_7_self_attn_k_proj_weight: PARAMETER target='layers.7.self_attn.k_proj.weight'\n",
       "            p_layers_7_self_attn_k_proj_bias: PARAMETER target='layers.7.self_attn.k_proj.bias'\n",
       "            p_layers_7_self_attn_v_proj_weight: PARAMETER target='layers.7.self_attn.v_proj.weight'\n",
       "            p_layers_7_self_attn_v_proj_bias: PARAMETER target='layers.7.self_attn.v_proj.bias'\n",
       "            p_layers_7_self_attn_q_proj_weight: PARAMETER target='layers.7.self_attn.q_proj.weight'\n",
       "            p_layers_7_self_attn_q_proj_bias: PARAMETER target='layers.7.self_attn.q_proj.bias'\n",
       "            p_layers_7_self_attn_out_proj_weight: PARAMETER target='layers.7.self_attn.out_proj.weight'\n",
       "            p_layers_7_self_attn_out_proj_bias: PARAMETER target='layers.7.self_attn.out_proj.bias'\n",
       "            p_layers_7_self_attn_layer_norm_weight: PARAMETER target='layers.7.self_attn_layer_norm.weight'\n",
       "            p_layers_7_self_attn_layer_norm_bias: PARAMETER target='layers.7.self_attn_layer_norm.bias'\n",
       "            p_layers_7_fc1_weight: PARAMETER target='layers.7.fc1.weight'\n",
       "            p_layers_7_fc1_bias: PARAMETER target='layers.7.fc1.bias'\n",
       "            p_layers_7_fc2_weight: PARAMETER target='layers.7.fc2.weight'\n",
       "            p_layers_7_fc2_bias: PARAMETER target='layers.7.fc2.bias'\n",
       "            p_layers_7_final_layer_norm_weight: PARAMETER target='layers.7.final_layer_norm.weight'\n",
       "            p_layers_7_final_layer_norm_bias: PARAMETER target='layers.7.final_layer_norm.bias'\n",
       "            p_layers_8_self_attn_k_proj_weight: PARAMETER target='layers.8.self_attn.k_proj.weight'\n",
       "            p_layers_8_self_attn_k_proj_bias: PARAMETER target='layers.8.self_attn.k_proj.bias'\n",
       "            p_layers_8_self_attn_v_proj_weight: PARAMETER target='layers.8.self_attn.v_proj.weight'\n",
       "            p_layers_8_self_attn_v_proj_bias: PARAMETER target='layers.8.self_attn.v_proj.bias'\n",
       "            p_layers_8_self_attn_q_proj_weight: PARAMETER target='layers.8.self_attn.q_proj.weight'\n",
       "            p_layers_8_self_attn_q_proj_bias: PARAMETER target='layers.8.self_attn.q_proj.bias'\n",
       "            p_layers_8_self_attn_out_proj_weight: PARAMETER target='layers.8.self_attn.out_proj.weight'\n",
       "            p_layers_8_self_attn_out_proj_bias: PARAMETER target='layers.8.self_attn.out_proj.bias'\n",
       "            p_layers_8_self_attn_layer_norm_weight: PARAMETER target='layers.8.self_attn_layer_norm.weight'\n",
       "            p_layers_8_self_attn_layer_norm_bias: PARAMETER target='layers.8.self_attn_layer_norm.bias'\n",
       "            p_layers_8_fc1_weight: PARAMETER target='layers.8.fc1.weight'\n",
       "            p_layers_8_fc1_bias: PARAMETER target='layers.8.fc1.bias'\n",
       "            p_layers_8_fc2_weight: PARAMETER target='layers.8.fc2.weight'\n",
       "            p_layers_8_fc2_bias: PARAMETER target='layers.8.fc2.bias'\n",
       "            p_layers_8_final_layer_norm_weight: PARAMETER target='layers.8.final_layer_norm.weight'\n",
       "            p_layers_8_final_layer_norm_bias: PARAMETER target='layers.8.final_layer_norm.bias'\n",
       "            p_layers_9_self_attn_k_proj_weight: PARAMETER target='layers.9.self_attn.k_proj.weight'\n",
       "            p_layers_9_self_attn_k_proj_bias: PARAMETER target='layers.9.self_attn.k_proj.bias'\n",
       "            p_layers_9_self_attn_v_proj_weight: PARAMETER target='layers.9.self_attn.v_proj.weight'\n",
       "            p_layers_9_self_attn_v_proj_bias: PARAMETER target='layers.9.self_attn.v_proj.bias'\n",
       "            p_layers_9_self_attn_q_proj_weight: PARAMETER target='layers.9.self_attn.q_proj.weight'\n",
       "            p_layers_9_self_attn_q_proj_bias: PARAMETER target='layers.9.self_attn.q_proj.bias'\n",
       "            p_layers_9_self_attn_out_proj_weight: PARAMETER target='layers.9.self_attn.out_proj.weight'\n",
       "            p_layers_9_self_attn_out_proj_bias: PARAMETER target='layers.9.self_attn.out_proj.bias'\n",
       "            p_layers_9_self_attn_layer_norm_weight: PARAMETER target='layers.9.self_attn_layer_norm.weight'\n",
       "            p_layers_9_self_attn_layer_norm_bias: PARAMETER target='layers.9.self_attn_layer_norm.bias'\n",
       "            p_layers_9_fc1_weight: PARAMETER target='layers.9.fc1.weight'\n",
       "            p_layers_9_fc1_bias: PARAMETER target='layers.9.fc1.bias'\n",
       "            p_layers_9_fc2_weight: PARAMETER target='layers.9.fc2.weight'\n",
       "            p_layers_9_fc2_bias: PARAMETER target='layers.9.fc2.bias'\n",
       "            p_layers_9_final_layer_norm_weight: PARAMETER target='layers.9.final_layer_norm.weight'\n",
       "            p_layers_9_final_layer_norm_bias: PARAMETER target='layers.9.final_layer_norm.bias'\n",
       "            p_layers_10_self_attn_k_proj_weight: PARAMETER target='layers.10.self_attn.k_proj.weight'\n",
       "            p_layers_10_self_attn_k_proj_bias: PARAMETER target='layers.10.self_attn.k_proj.bias'\n",
       "            p_layers_10_self_attn_v_proj_weight: PARAMETER target='layers.10.self_attn.v_proj.weight'\n",
       "            p_layers_10_self_attn_v_proj_bias: PARAMETER target='layers.10.self_attn.v_proj.bias'\n",
       "            p_layers_10_self_attn_q_proj_weight: PARAMETER target='layers.10.self_attn.q_proj.weight'\n",
       "            p_layers_10_self_attn_q_proj_bias: PARAMETER target='layers.10.self_attn.q_proj.bias'\n",
       "            p_layers_10_self_attn_out_proj_weight: PARAMETER target='layers.10.self_attn.out_proj.weight'\n",
       "            p_layers_10_self_attn_out_proj_bias: PARAMETER target='layers.10.self_attn.out_proj.bias'\n",
       "            p_layers_10_self_attn_layer_norm_weight: PARAMETER target='layers.10.self_attn_layer_norm.weight'\n",
       "            p_layers_10_self_attn_layer_norm_bias: PARAMETER target='layers.10.self_attn_layer_norm.bias'\n",
       "            p_layers_10_fc1_weight: PARAMETER target='layers.10.fc1.weight'\n",
       "            p_layers_10_fc1_bias: PARAMETER target='layers.10.fc1.bias'\n",
       "            p_layers_10_fc2_weight: PARAMETER target='layers.10.fc2.weight'\n",
       "            p_layers_10_fc2_bias: PARAMETER target='layers.10.fc2.bias'\n",
       "            p_layers_10_final_layer_norm_weight: PARAMETER target='layers.10.final_layer_norm.weight'\n",
       "            p_layers_10_final_layer_norm_bias: PARAMETER target='layers.10.final_layer_norm.bias'\n",
       "            p_layers_11_self_attn_k_proj_weight: PARAMETER target='layers.11.self_attn.k_proj.weight'\n",
       "            p_layers_11_self_attn_k_proj_bias: PARAMETER target='layers.11.self_attn.k_proj.bias'\n",
       "            p_layers_11_self_attn_v_proj_weight: PARAMETER target='layers.11.self_attn.v_proj.weight'\n",
       "            p_layers_11_self_attn_v_proj_bias: PARAMETER target='layers.11.self_attn.v_proj.bias'\n",
       "            p_layers_11_self_attn_q_proj_weight: PARAMETER target='layers.11.self_attn.q_proj.weight'\n",
       "            p_layers_11_self_attn_q_proj_bias: PARAMETER target='layers.11.self_attn.q_proj.bias'\n",
       "            p_layers_11_self_attn_out_proj_weight: PARAMETER target='layers.11.self_attn.out_proj.weight'\n",
       "            p_layers_11_self_attn_out_proj_bias: PARAMETER target='layers.11.self_attn.out_proj.bias'\n",
       "            p_layers_11_self_attn_layer_norm_weight: PARAMETER target='layers.11.self_attn_layer_norm.weight'\n",
       "            p_layers_11_self_attn_layer_norm_bias: PARAMETER target='layers.11.self_attn_layer_norm.bias'\n",
       "            p_layers_11_fc1_weight: PARAMETER target='layers.11.fc1.weight'\n",
       "            p_layers_11_fc1_bias: PARAMETER target='layers.11.fc1.bias'\n",
       "            p_layers_11_fc2_weight: PARAMETER target='layers.11.fc2.weight'\n",
       "            p_layers_11_fc2_bias: PARAMETER target='layers.11.fc2.bias'\n",
       "            p_layers_11_final_layer_norm_weight: PARAMETER target='layers.11.final_layer_norm.weight'\n",
       "            p_layers_11_final_layer_norm_bias: PARAMETER target='layers.11.final_layer_norm.bias'\n",
       "            p_layer_norm_weight: PARAMETER target='layer_norm.weight'\n",
       "            p_layer_norm_bias: PARAMETER target='layer_norm.bias'\n",
       "            input_features: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            layer_norm_24: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s6: VR[0, int_oo], s37: VR[0, 24000]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = model.model.encoder\n",
    "dummy_input = torch.randn(1, 500, processor.feature_extractor.feature_size)  # [batch, time, mel_dim]\n",
    "\n",
    "torch.onnx.export(\n",
    "    encoder,\n",
    "    dummy_input,\n",
    "    \"s2t_encoder.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_features\"],\n",
    "    output_names=[\"encoder_outputs\"],\n",
    "    dynamic_axes={\n",
    "        \"input_features\": {0: \"batch\", 1: \"time\"},\n",
    "        \"encoder_outputs\": {0: \"batch\", 1: \"time\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984d984d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12824\\1251177408.py\", line 3, in <module>\n",
      "    from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\optimum\\onnxruntime\\__init__.py\", line 16, in <module>\n",
      "    from transformers.utils import OptionalDependencyNotAvailable, _LazyModule\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\utils\\generic.py\", line 53, in <module>\n",
      "    import torch  # noqa: F401\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "#--- Converting the decoder:\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq\n",
    "from transformers import Speech2TextProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798cb2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export the model.\n",
      "The task `automatic-speech-recognition` was manually specified, and past key values will not be reused in the decoding. if needed, please pass `--task automatic-speech-recognition-with-past` to export using the past key values.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\configuration_utils.py:448: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}\n",
      "  warnings.warn(\n",
      "Using framework PyTorch: 2.1.0+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\models\\speech_to_text\\modeling_speech_to_text.py:156: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_pos > self.weights.size(0):\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
      "Using framework PyTorch: 2.1.0+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "c:\\Users\\User\\dev\\SpeechToText\\backend\\.venv310\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:170: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"facebook/s2t-small-librispeech-asr\"\n",
    "\n",
    "processor = Speech2TextProcessor.from_pretrained(model_id)\n",
    "\n",
    "onnx_model = ORTModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    export=True,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "onnx_model.save_pretrained(\"onnx_s2t\")\n",
    "processor.save_pretrained(\"onnx_s2t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add51ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
